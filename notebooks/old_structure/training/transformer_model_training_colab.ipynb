{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Transformer Model Training (Google Colabç‰ˆ)\n",
        "## æš—å·é€šè²¨ä¾¡æ ¼äºˆæ¸¬ã®ãŸã‚ã®Transformerãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\n",
        "\n",
        "ã“ã®Notebookã¯ã€Google Colabç’°å¢ƒã§Transformerãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹ãŸã‚ã®ã‚‚ã®ã§ã™ã€‚\n",
        "\n",
        "âš ï¸ **äº‹å‰æº–å‚™**\n",
        "1. `google_colab_setup.ipynb`ã‚’å®Ÿè¡Œã—ã¦ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’å®Œäº†ã—ã¦ãã ã•ã„\n",
        "2. KuCoin APIã‚­ãƒ¼ã‚’Google Colabã®Secretsã«è¨­å®šã—ã¦ãã ã•ã„\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Google Colabç’°å¢ƒã®è¨­å®šã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "import os\n",
        "\n",
        "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•\n",
        "if '/content/sakata_goho_chart' not in os.getcwd():\n",
        "    os.chdir('/content/sakata_goho_chart')\n",
        "\n",
        "print(f\"ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {os.getcwd()}\")\n",
        "\n",
        "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# è‡ªä½œãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "from src.data_fetcher import DataFetcher\n",
        "from src.technical_indicators import TechnicalIndicators\n",
        "from src.utils import DataPreprocessor, ModelUtils\n",
        "\n",
        "# Transformerãƒ¢ãƒ‡ãƒ«ç”¨ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LayerNormalization, Dropout\n",
        "from tensorflow.keras.layers import MultiHeadAttention, GlobalAveragePooling1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒ‡ãƒ¼ã‚¿å–å¾—\n",
        "from google.colab import userdata\n",
        "\n",
        "# KuCoin APIã®è¨­å®š\n",
        "fetcher = DataFetcher(\n",
        "    exchange_name='kucoin',\n",
        "    api_key=userdata.get('KuCoin_API_KEY'),\n",
        "    secret=userdata.get('KuCoin_API_SECRET'),\n",
        "    password=userdata.get('KuCoin_API_PASSPHRAS')\n",
        ")\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿å–å¾—è¨­å®š\n",
        "symbol = 'SOL/USDT'\n",
        "timeframe = '1d'\n",
        "limit = 8760 * 2  # 2å¹´åˆ†\n",
        "\n",
        "print(f\"ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹: {symbol}\")\n",
        "data = fetcher.fetch_ohlcv_data(symbol, timeframe, limit)\n",
        "\n",
        "if data is not None:\n",
        "    print(f\"å–å¾—å®Œäº†: {len(data)}ä»¶\")\n",
        "    print(data.head())\n",
        "    \n",
        "    # Google Driveã«ä¿å­˜ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç”¨ï¼‰\n",
        "    backup_path = \"/content/drive/MyDrive/trading_bot_data.csv\"\n",
        "    data.to_csv(backup_path)\n",
        "    print(f\"ãƒ‡ãƒ¼ã‚¿ã‚’Google Driveã«ä¿å­˜: {backup_path}\")\n",
        "else:\n",
        "    print(\"ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã®è¿½åŠ \n",
        "print(\"ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã‚’è¨ˆç®—ä¸­...\")\n",
        "df = TechnicalIndicators.add_all_indicators(data)\n",
        "print(f\"ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™è¿½åŠ å®Œäº†: {df.shape}\")\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦ã‚’è¡¨ç¤º\n",
        "print(\"\\nãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬çµ±è¨ˆ:\")\n",
        "print(df.describe())\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã‚’å¯è¦–åŒ–\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(df.index, df['Close'])\n",
        "plt.title('Price Chart')\n",
        "plt.ylabel('Price')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(df.index, df['RSI'])\n",
        "plt.title('RSI')\n",
        "plt.ylabel('RSI')\n",
        "plt.axhline(y=70, color='r', linestyle='--', alpha=0.5)\n",
        "plt.axhline(y=30, color='r', linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(df.index, df['MACD'])\n",
        "plt.title('MACD')\n",
        "plt.ylabel('MACD')\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(df.index, df['Volume'])\n",
        "plt.title('Volume')\n",
        "plt.ylabel('Volume')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transformerãƒ¢ãƒ‡ãƒ«ã®å®šç¾©ï¼ˆGoogle Colabç”¨ï¼‰\n",
        "def create_transformer_model(input_dim, num_heads=8, ff_dim=32, num_layers=2, dropout_rate=0.1):\n",
        "    \"\"\"\n",
        "    Transformerãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ\n",
        "    \n",
        "    Args:\n",
        "        input_dim: å…¥åŠ›æ¬¡å…ƒæ•°\n",
        "        num_heads: ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®ãƒ˜ãƒƒãƒ‰æ•°\n",
        "        ff_dim: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã®æ¬¡å…ƒæ•°\n",
        "        num_layers: Transformerãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æ•°\n",
        "        dropout_rate: ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡\n",
        "    \n",
        "    Returns:\n",
        "        Kerasãƒ¢ãƒ‡ãƒ«\n",
        "    \"\"\"\n",
        "    inputs = Input(shape=(1, input_dim))\n",
        "    \n",
        "    # Transformerãƒ–ãƒ­ãƒƒã‚¯\n",
        "    x = inputs\n",
        "    for _ in range(num_layers):\n",
        "        # Multi-Head Attention\n",
        "        attention_output = MultiHeadAttention(\n",
        "            num_heads=num_heads, \n",
        "            key_dim=input_dim // num_heads\n",
        "        )(x, x)\n",
        "        attention_output = Dropout(dropout_rate)(attention_output)\n",
        "        x = LayerNormalization(epsilon=1e-6)(x + attention_output)\n",
        "        \n",
        "        # Feed Forward\n",
        "        ffn_output = Dense(ff_dim, activation=\"relu\")(x)\n",
        "        ffn_output = Dense(input_dim)(ffn_output)\n",
        "        ffn_output = Dropout(dropout_rate)(ffn_output)\n",
        "        x = LayerNormalization(epsilon=1e-6)(x + ffn_output)\n",
        "    \n",
        "    # Global Average Pooling\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    \n",
        "    # å‡ºåŠ›å±¤\n",
        "    x = Dense(64, activation=\"relu\")(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
        "    \n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "print(\"Transformerãƒ¢ãƒ‡ãƒ«å®šç¾©å®Œäº†\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†\n",
        "print(\"ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†é–‹å§‹...\")\n",
        "preprocessor = DataPreprocessor(scaler_type='standard')\n",
        "X_scaled, y = preprocessor.fit_transform(df, target_column='Close')\n",
        "\n",
        "# æœ€å¾Œã®è¡Œã¯äºˆæ¸¬å¯¾è±¡ãªã®ã§é™¤å¤–\n",
        "X_scaled = X_scaled[:-1]\n",
        "y = y[:-1]\n",
        "\n",
        "print(f\"ç‰¹å¾´é‡æ•°: {X_scaled.shape[1]}\")\n",
        "print(f\"ãƒ‡ãƒ¼ã‚¿æ•°: {X_scaled.shape[0]}\")\n",
        "print(f\"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ: {np.bincount(y)}\")\n",
        "\n",
        "# å­¦ç¿’ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Transformerãƒ¢ãƒ‡ãƒ«ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’3æ¬¡å…ƒã«å¤‰æ›\n",
        "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
        "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
        "\n",
        "print(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X_train.shape}\")\n",
        "print(f\"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test.shape}\")\n",
        "\n",
        "# GPUä½¿ç”¨çŠ¶æ³ã®ç¢ºèª\n",
        "print(f\"\\nGPUä½¿ç”¨å¯èƒ½: {tf.config.list_physical_devices('GPU')}\")\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print(\"âœ… GPUãŒä½¿ç”¨å¯èƒ½ã§ã™\")\n",
        "else:\n",
        "    print(\"âš ï¸ CPUã§å®Ÿè¡Œã•ã‚Œã¾ã™\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã¨ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«\n",
        "print(\"ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã¨ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«...\")\n",
        "model = create_transformer_model(input_dim=X_train.shape[2])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã®æ¦‚è¦ã‚’è¡¨ç¤º\n",
        "model.summary()\n",
        "\n",
        "# Google Colabã§ã®è¨“ç·´é€²æ—ã‚’ç›£è¦–ã™ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Google Driveã«ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ‘ã‚¹\n",
        "checkpoint_path = \"/content/drive/MyDrive/transformer_model_checkpoint.h5\"\n",
        "\n",
        "callbacks = [\n",
        "    ModelCheckpoint(\n",
        "        checkpoint_path,\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        mode='max',\n",
        "        verbose=1\n",
        "    ),\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"ãƒ¢ãƒ‡ãƒ«æº–å‚™å®Œäº†\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´é–‹å§‹\n",
        "print(\"ğŸš€ ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
        "print(\"â±ï¸ è¨“ç·´æ™‚é–“ã¯ç´„15-30åˆ†ç¨‹åº¦ã‚’äºˆæƒ³ã—ã¦ã„ã¾ã™\")\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100,  # å¤šã‚ã«è¨­å®šï¼ˆEarlyStoppingã§è‡ªå‹•åœæ­¢ï¼‰\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "print(f\"âœ… è¨“ç·´å®Œäº†ï¼\")\n",
        "print(f\"â±ï¸ è¨“ç·´æ™‚é–“: {training_time/60:.1f}åˆ†\")\n",
        "\n",
        "# è¨“ç·´çµæœã®å¯è¦–åŒ–\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss', alpha=0.8)\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', alpha=0.8)\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy', alpha=0.8)\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', alpha=0.8)\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(history.history['lr'] if 'lr' in history.history else [], label='Learning Rate', alpha=0.8)\n",
        "plt.title('Learning Rate')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã¨ä¿å­˜\n",
        "print(\"ğŸ“Š ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ä¸­...\")\n",
        "\n",
        "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"âœ… ãƒ†ã‚¹ãƒˆç²¾åº¦: {test_accuracy:.4f}\")\n",
        "print(f\"ğŸ“‰ ãƒ†ã‚¹ãƒˆæå¤±: {test_loss:.4f}\")\n",
        "\n",
        "# äºˆæ¸¬çµæœã®è©³ç´°åˆ†æ\n",
        "y_pred = model.predict(X_test, verbose=0)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(\"\\nğŸ“‹ åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:\")\n",
        "print(classification_report(y_test, y_pred_binary))\n",
        "\n",
        "print(\"\\nğŸ¯ æ··åŒè¡Œåˆ—:\")\n",
        "cm = confusion_matrix(y_test, y_pred_binary)\n",
        "print(cm)\n",
        "\n",
        "# æ··åŒè¡Œåˆ—ã®å¯è¦–åŒ–\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.colorbar()\n",
        "classes = ['ä¸‹è½', 'ä¸Šæ˜‡']\n",
        "tick_marks = np.arange(len(classes))\n",
        "plt.xticks(tick_marks, classes)\n",
        "plt.yticks(tick_marks, classes)\n",
        "\n",
        "# æ•°å€¤ã‚’è¡¨ç¤º\n",
        "thresh = cm.max() / 2.\n",
        "for i, j in np.ndindex(cm.shape):\n",
        "    plt.text(j, i, format(cm[i, j], 'd'),\n",
        "             horizontalalignment=\"center\",\n",
        "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.ylabel('å®Ÿéš›ã®ãƒ©ãƒ™ãƒ«')\n",
        "plt.xlabel('äºˆæ¸¬ãƒ©ãƒ™ãƒ«')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Google Driveã«ãƒ¢ãƒ‡ãƒ«ã¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’ä¿å­˜\n",
        "final_model_path = \"/content/drive/MyDrive/transformer_model_final.h5\"\n",
        "scaler_path = \"/content/drive/MyDrive/scaler.pkl\"\n",
        "\n",
        "model.save(final_model_path)\n",
        "preprocessor.save_scaler(scaler_path)\n",
        "\n",
        "print(f\"ğŸ’¾ æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜: {final_model_path}\")\n",
        "print(f\"ğŸ’¾ ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’ä¿å­˜: {scaler_path}\")\n",
        "\n",
        "# è¨“ç·´ã‚µãƒãƒªãƒ¼ã®ä¿å­˜\n",
        "import json\n",
        "training_summary = {\n",
        "    'symbol': symbol,\n",
        "    'timeframe': timeframe,\n",
        "    'data_points': len(data),\n",
        "    'features': X_train.shape[2],\n",
        "    'training_time_minutes': training_time/60,\n",
        "    'final_accuracy': float(test_accuracy),\n",
        "    'final_loss': float(test_loss),\n",
        "    'epochs_trained': len(history.history['loss'])\n",
        "}\n",
        "\n",
        "with open(\"/content/drive/MyDrive/training_summary.json\", \"w\") as f:\n",
        "    json.dump(training_summary, f, indent=2)\n",
        "\n",
        "print(\"ğŸ“ è¨“ç·´ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜ã—ã¾ã—ãŸ\")\n",
        "print(\"\\nğŸ‰ å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
