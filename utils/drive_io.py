"""
Google Drive ãƒžã‚¦ãƒ³ãƒˆ & å…±é€šå…¥å‡ºåŠ›ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£

Colab ã§ã®ä½¿ç”¨æ–¹æ³•:
    sys.path.append('/content/drive/MyDrive/kucoin_bot')
    from utils.drive_io import mount_drive, save_data, load_data, DataFetcher
"""

import pandas as pd
import numpy as np
import ccxt
import pickle
import os
import time
from typing import Tuple, Optional, Dict, Any
from sklearn.preprocessing import StandardScaler, MinMaxScaler


def mount_drive():
    """Google Drive ã‚’ãƒžã‚¦ãƒ³ãƒˆ (Colabå°‚ç”¨)"""
    try:
        from google.colab import drive
        drive.mount('/content/drive')
        print("Google Drive ãŒãƒžã‚¦ãƒ³ãƒˆã•ã‚Œã¾ã—ãŸ")
        return True
    except ImportError:
        print("Google Colab ç’°å¢ƒã§ã¯ã‚ã‚Šã¾ã›ã‚“")
        return False


def get_project_path():
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã‚’å–å¾—"""
    if os.path.exists('/content/drive/MyDrive/kucoin_bot'):
        return '/content/drive/MyDrive/kucoin_bot'
    else:
        return '.'


def load_raw(symbol: str = "SOL_USDT", timeframe: str = "1d", limit: int = 1000) -> pd.DataFrame:
    """
    ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ data/raw ã‹ã‚‰èª­ã¿è¾¼ã¿
    
    Args:
        symbol: é€šè²¨ãƒšã‚¢ (ä¾‹: 'SOL_USDT')
        timeframe: æ™‚é–“è¶³ (ä¾‹: '1d', '1h')
        limit: ãƒ‡ãƒ¼ã‚¿æ•°
        
    Returns:
        pd.DataFrame: OHLCVãƒ‡ãƒ¼ã‚¿
    """
    project_path = get_project_path()
    raw_data_dir = os.path.join(project_path, "data", "raw")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åã®æ§‹ç¯‰
    filename = f"{symbol}_{timeframe}_{limit}.csv"
    filepath = os.path.join(raw_data_dir, filename)
    
    if not os.path.exists(filepath):
        # Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚‚è©¦ã™
        parquet_filename = f"{symbol}_{timeframe}_{limit}.parquet"
        parquet_filepath = os.path.join(raw_data_dir, parquet_filename)
        
        if os.path.exists(parquet_filepath):
            df = pd.read_parquet(parquet_filepath)
        else:
            raise FileNotFoundError(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filepath} ã¾ãŸã¯ {parquet_filepath}")
    else:
        df = pd.read_csv(filepath, index_col=0, parse_dates=True)
    
    print(f"âœ… ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {filepath}")
    print(f"ðŸ“Š ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df.shape}")
    print(f"ðŸ“… æœŸé–“: {df.index[0]} ï½ž {df.index[-1]}")
    
    return df


def save_data(data: Any, filename: str, data_dir: str = "data") -> None:
    """
    ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    
    Args:
        data: ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿
        filename: ãƒ•ã‚¡ã‚¤ãƒ«å
        data_dir: ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    """
    project_path = get_project_path()
    save_path = os.path.join(project_path, data_dir)
    os.makedirs(save_path, exist_ok=True)
    
    filepath = os.path.join(save_path, filename)
    
    if filename.endswith('.csv'):
        data.to_csv(filepath, index=True)
    elif filename.endswith('.pkl'):
        with open(filepath, 'wb') as f:
            pickle.dump(data, f)
    else:
        raise ValueError("ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼")
    
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")


def load_data(filename: str, data_dir: str = "data") -> Any:
    """
    ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    
    Args:
        filename: ãƒ•ã‚¡ã‚¤ãƒ«å
        data_dir: ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        
    Returns:
        èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿
    """
    project_path = get_project_path()
    filepath = os.path.join(project_path, data_dir, filename)
    
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filepath}")
    
    if filename.endswith('.csv'):
        return pd.read_csv(filepath, index_col=0, parse_dates=True)
    elif filename.endswith('.pkl'):
        with open(filepath, 'rb') as f:
            return pickle.load(f)
    else:
        raise ValueError("ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼")


def save_df(df: pd.DataFrame, path: str) -> None:
    """
    DataFrameã‚’ä¿å­˜ (Issue #4 è¦ä»¶å¯¾å¿œ)
    
    Args:
        df: ä¿å­˜ã™ã‚‹DataFrame
        path: ä¿å­˜ãƒ‘ã‚¹
    """
    # ãƒ‘ã‚¹ã‹ã‚‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    os.makedirs(os.path.dirname(path) if os.path.dirname(path) else '.', exist_ok=True)
    
    if path.endswith('.csv'):
        df.to_csv(path, index=True)
    elif path.endswith('.parquet'):
        df.to_parquet(path, index=True)
    elif path.endswith('.pkl'):
        with open(path, 'wb') as f:
            pickle.dump(df, f)
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯CSV
        if not path.endswith('.csv'):
            path += '.csv'
        df.to_csv(path, index=True)
    
    print(f"DataFrameã‚’ä¿å­˜ã—ã¾ã—ãŸ: {path}")


def load_df(path: str) -> pd.DataFrame:
    """
    DataFrameã‚’èª­ã¿è¾¼ã¿ (Issue #4 è¦ä»¶å¯¾å¿œ)
    
    Args:
        path: èª­ã¿è¾¼ã¿ãƒ‘ã‚¹
        
    Returns:
        pd.DataFrame: èª­ã¿è¾¼ã‚“ã DataFrame
    """
    if not os.path.exists(path):
        raise FileNotFoundError(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}")
    
    if path.endswith('.csv'):
        return pd.read_csv(path, index_col=0, parse_dates=True)
    elif path.endswith('.parquet'):
        return pd.read_parquet(path)
    elif path.endswith('.pkl'):
        with open(path, 'rb') as f:
            return pickle.load(f)
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯CSVã¨ã—ã¦èª­ã¿è¾¼ã¿
        return pd.read_csv(path, index_col=0, parse_dates=True)


class DataFetcher:
    """æš—å·é€šè²¨ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¯ãƒ©ã‚¹"""

    def __init__(
        self,
        exchange_name: str = "kucoin",
        api_key: str = None,
        secret: str = None,
        password: str = None,
    ):
        """
        å–å¼•æ‰€ã®åˆæœŸåŒ–

        Args:
            exchange_name: å–å¼•æ‰€å
            api_key: APIã‚­ãƒ¼
            secret: APIã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆ
            password: APIãƒ‘ã‚¹ãƒ•ãƒ¬ãƒ¼ã‚º
        """
        self.exchange_name = exchange_name
        self.exchange = None

        if exchange_name.lower() == "kucoin":
            self.exchange = ccxt.kucoin(
                {
                    "apiKey": api_key,
                    "secret": secret,
                    "password": password,
                    "enableRateLimit": True,
                }
            )

    def fetch_ohlcv_data(
        self, symbol: str, timeframe: str = "1d", limit: int = 1000
    ) -> pd.DataFrame:
        """
        OHLCVãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

        Args:
            symbol: é€šè²¨ãƒšã‚¢ (ä¾‹: 'BTC/USDT')
            timeframe: æ™‚é–“è¶³ (ä¾‹: '1d', '1h', '4h')
            limit: å–å¾—ã™ã‚‹ãƒ‡ãƒ¼ã‚¿æ•°

        Returns:
            pd.DataFrame: OHLCVãƒ‡ãƒ¼ã‚¿
        """
        try:
            ohlcv = self.exchange.fetch_ohlcv(symbol, timeframe, limit=limit)
            data = pd.DataFrame(
                ohlcv, columns=["timestamp", "open", "high", "low", "close", "volume"]
            )
            data = data.rename(
                columns={
                    "open": "Open",
                    "high": "High",
                    "low": "Low",
                    "close": "Close",
                    "volume": "Volume",
                }
            )
            data["timestamp"] = pd.to_datetime(data["timestamp"], unit="ms")
            data.set_index("timestamp", inplace=True)

            print(f"ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {symbol}, {len(data)}ä»¶")
            return data

        except Exception as e:
            print(f"ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def fetch_multiple_symbols(
        self, symbols: list, timeframe: str = "1d", limit: int = 1000
    ) -> dict:
        """
        è¤‡æ•°ã®é€šè²¨ãƒšã‚¢ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

        Args:
            symbols: é€šè²¨ãƒšã‚¢ã®ãƒªã‚¹ãƒˆ
            timeframe: æ™‚é–“è¶³
            limit: å–å¾—ã™ã‚‹ãƒ‡ãƒ¼ã‚¿æ•°

        Returns:
            dict: {symbol: DataFrame}
        """
        data_dict = {}

        for symbol in symbols:
            data = self.fetch_ohlcv_data(symbol, timeframe, limit)
            if data is not None:
                data_dict[symbol] = data
            time.sleep(1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–

        return data_dict


class ModelUtils:
    """ãƒ¢ãƒ‡ãƒ«é–¢é€£ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹"""

    @staticmethod
    def save_model(model, model_name: str, model_dir: str = "models"):
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜

        Args:
            model: ä¿å­˜ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
            model_name: ãƒ¢ãƒ‡ãƒ«å
            model_dir: ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        project_path = get_project_path()
        model_path = os.path.join(project_path, model_dir, model_name)
        os.makedirs(os.path.dirname(model_path), exist_ok=True)

        if hasattr(model, "save"):
            # Kerasãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
            model.save(model_path)
        else:
            # scikit-learnãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
            with open(model_path, "wb") as f:
                pickle.dump(model, f)

        print(f"ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {model_path}")

    @staticmethod
    def load_model(model_name: str, model_dir: str = "models"):
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿

        Args:
            model_name: ãƒ¢ãƒ‡ãƒ«å
            model_dir: ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

        Returns:
            èª­ã¿è¾¼ã‚“ã ãƒ¢ãƒ‡ãƒ«
        """
        project_path = get_project_path()
        model_path = os.path.join(project_path, model_dir, model_name)
        
        if model_name.endswith(".h5") or model_name.endswith(".keras"):
            # Kerasãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
            from tensorflow.keras.models import load_model
            return load_model(model_path)
        else:
            # scikit-learnãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
            with open(model_path, "rb") as f:
                return pickle.load(f)


class DataPreprocessor:
    """ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self, scaler_type: str = "standard"):
        """
        åˆæœŸåŒ–

        Args:
            scaler_type: ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã®ç¨®é¡ž ('standard' or 'minmax')
        """
        self.scaler_type = scaler_type
        self.scaler = StandardScaler() if scaler_type == "standard" else MinMaxScaler()
        self.feature_columns = None

    def fit_transform(self, df: pd.DataFrame, target_column: str = "Close") -> tuple:
        """
        ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆ

        Args:
            df: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
            target_column: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—å

        Returns:
            tuple: (X_scaled, y)
        """
        # ç‰¹å¾´é‡åˆ—ã‚’ç‰¹å®š
        self.feature_columns = [col for col in df.columns if col not in [target_column]]

        # ç‰¹å¾´é‡ã®æŠ½å‡º
        X = df[self.feature_columns].fillna(0)

        # æ­£è¦åŒ–
        X_scaled = self.scaler.fit_transform(X)

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ä½œæˆï¼ˆæ¬¡ã®æ—¥ã®ä¾¡æ ¼å¤‰å‹•æ–¹å‘ï¼‰
        y = (df[target_column].shift(-1) > df[target_column]).astype(int)

        return X_scaled, y.values

    def transform(self, df: pd.DataFrame) -> np.ndarray:
        """
        å­¦ç¿’æ¸ˆã¿ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã§ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›

        Args:
            df: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 

        Returns:
            np.ndarray: æ­£è¦åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
        """
        X = df[self.feature_columns].fillna(0)
        return self.scaler.transform(X)

    def save_scaler(self, filename: str = "scaler.pkl"):
        """ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’ä¿å­˜"""
        save_data(self.scaler, filename, "models")

    def load_scaler(self, filename: str = "scaler.pkl"):
        """ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’èª­ã¿è¾¼ã¿"""
        self.scaler = load_data(filename, "models")


class BacktestUtils:
    """ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆé–¢é€£ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹"""

    @staticmethod
    def calculate_returns(
        df: pd.DataFrame, signal_column: str = "signal"
    ) -> pd.DataFrame:
        """
        ãƒªã‚¿ãƒ¼ãƒ³ã‚’è¨ˆç®—

        Args:
            df: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
            signal_column: ã‚·ã‚°ãƒŠãƒ«åˆ—å

        Returns:
            pd.DataFrame: ãƒªã‚¿ãƒ¼ãƒ³ã‚’è¿½åŠ ã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        """
        df = df.copy()

        # æ—¥æ¬¡ãƒªã‚¿ãƒ¼ãƒ³
        df["returns"] = df["Close"].pct_change()

        # æˆ¦ç•¥ãƒªã‚¿ãƒ¼ãƒ³
        df["strategy_returns"] = df[signal_column].shift(1) * df["returns"]

        # ç´¯ç©ãƒªã‚¿ãƒ¼ãƒ³
        df["cumulative_returns"] = (1 + df["returns"]).cumprod()
        df["cumulative_strategy_returns"] = (1 + df["strategy_returns"]).cumprod()

        return df

    @staticmethod
    def calculate_metrics(returns: pd.Series) -> dict:
        """
        ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æŒ‡æ¨™ã‚’è¨ˆç®—

        Args:
            returns: ãƒªã‚¿ãƒ¼ãƒ³ã‚·ãƒªãƒ¼ã‚º

        Returns:
            dict: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æŒ‡æ¨™
        """
        total_return = (1 + returns).prod() - 1
        annual_return = (1 + returns).prod() ** (252 / len(returns)) - 1
        volatility = returns.std() * np.sqrt(252)
        sharpe_ratio = annual_return / volatility if volatility != 0 else 0
        max_drawdown = (returns.cumsum() - returns.cumsum().cummax()).min()

        return {
            "total_return": total_return,
            "annual_return": annual_return,
            "volatility": volatility,
            "sharpe_ratio": sharpe_ratio,
            "max_drawdown": max_drawdown,
        }