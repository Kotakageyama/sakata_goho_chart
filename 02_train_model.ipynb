# ã‚»ãƒ« 8: ãƒ¡ã‚¿æƒ…å ± JSON å‡ºåŠ›

# 6. æ¬¡å·¥ç¨‹ï¼ˆãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆï¼‰ã¸å¼•ãç¶™ããŸã‚ã®ãƒ¡ã‚¿æƒ…å ± JSON å‡ºåŠ›
print("ğŸ“‹ ãƒ¡ã‚¿æƒ…å ±ã‚’ä½œæˆä¸­...")

# ãƒ¡ã‚¿æƒ…å ±ã®ä½œæˆ
metadata = {
    "model_info": {
        "model_type": selected_model,
        "model_path": model_path,
        "feature_list_path": feature_list_path,
        "scaler_path": scaler_path if scaler is not None else None,
        "creation_date": today,
        "creation_datetime": datetime.now().isoformat()
    },
    "data_info": {
        "symbol": "SOL_USDT",
        "timeframe": "1d",
        "prediction_days": prediction_days,
        "total_samples": len(df_ml),
        "train_samples": len(X_train),
        "test_samples": len(X_test),
        "feature_count": len(feature_columns),
        "target_distribution": {
            "down_count": int(target_counts[0]),
            "up_count": int(target_counts[1]),
            "positive_rate": float(y.mean())
        }
    },
    "model_performance": {
        "accuracy": float(accuracy),
        "precision": float(precision),
        "recall": float(recall),
        "f1_score": float(f1),
        "auc": float(auc)
    },
    "training_parameters": {
        "test_size": float(test_size),
        "random_state": 42
    },
    "feature_categories": {
        "basic_ohlcv": [col for col in feature_columns if col in ['Open', 'High', 'Low', 'Close', 'Volume']],
        "technical_indicators": [col for col in feature_columns if any(indicator in col for indicator in ['SMA', 'RSI', 'MACD', 'ATR', 'bb_'])],
        "lag_features": [col for col in feature_columns if 'lag' in col],
        "pattern_features": [col for col in feature_columns if col in ['doji', 'hammer', 'shooting_star', 'engulfing_bullish', 'engulfing_bearish', 'three_white_soldiers', 'three_black_crows']]
    }
}

# ãƒ¡ã‚¿æƒ…å ±ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
metadata_path = os.path.join(model_dir, "metadata.json")
with open(metadata_path, 'w', encoding='utf-8') as f:
    json.dump(metadata, f, ensure_ascii=False, indent=2)

print(f"âœ… ãƒ¡ã‚¿æƒ…å ±ä¿å­˜å®Œäº†: {metadata_path}")

# çµæœã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º
print(f"\nğŸ‰ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ã‚µãƒãƒªãƒ¼:")
print(f"  - ãƒ¢ãƒ‡ãƒ«: {selected_model.upper()}")
print(f"  - ç²¾åº¦: {accuracy:.3f}")
print(f"  - F1ã‚¹ã‚³ã‚¢: {f1:.3f}")
print(f"  - AUC: {auc:.3f}")
print(f"  - ä¿å­˜å…ˆ: {model_dir}")
print(f"  - æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: 03_backtest.ipynb ã§ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")

# ãƒ¡ã‚¿æƒ…å ±ã®å†…å®¹ã‚’è¡¨ç¤º
print(f"\nğŸ“‹ ä¿å­˜ã•ã‚ŒãŸãƒ¡ã‚¿æƒ…å ±:")
display(pd.DataFrame([
    {"é …ç›®": "ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—", "å€¤": metadata["model_info"]["model_type"]},
    {"é …ç›®": "äºˆæ¸¬æœŸé–“", "å€¤": f"{metadata['data_info']['prediction_days']}æ—¥å¾Œ"},
    {"é …ç›®": "ã‚µãƒ³ãƒ—ãƒ«æ•°", "å€¤": metadata["data_info"]["total_samples"]},
    {"é …ç›®": "ç‰¹å¾´é‡æ•°", "å€¤": metadata["data_info"]["feature_count"]},
    {"é …ç›®": "æ­£ä¾‹ç‡", "å€¤": f"{metadata['data_info']['target_distribution']['positive_rate']:.3f}"},
    {"é …ç›®": "F1ã‚¹ã‚³ã‚¢", "å€¤": f"{metadata['model_performance']['f1_score']:.3f}"},
    {"é …ç›®": "AUC", "å€¤": f"{metadata['model_performance']['auc']:.3f}"}
]))# ã‚»ãƒ« 7: ãƒ¢ãƒ‡ãƒ«ä¿å­˜

# 5. å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ GDrive /data/models/{date}/model.pkl ã«ä¿å­˜
print("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ä¸­...")

# æ—¥ä»˜ãƒ•ã‚©ãƒ«ãƒ€ã®ä½œæˆ
today = datetime.now().strftime("%Y%m%d")
project_path = get_project_path()
model_dir = os.path.join(project_path, "data", "models", today)
os.makedirs(model_dir, exist_ok=True)

# ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‘ã‚¹
model_path = os.path.join(model_dir, "model.pkl")

# ãƒ¢ãƒ‡ãƒ«ä¿å­˜
import pickle
with open(model_path, 'wb') as f:
    pickle.dump(model, f)

print(f"âœ… ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {model_path}")

# ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã‚‚ä¿å­˜
feature_list_path = os.path.join(model_dir, "features.json")
with open(feature_list_path, 'w', encoding='utf-8') as f:
    json.dump(feature_columns, f, ensure_ascii=False, indent=2)

print(f"âœ… ç‰¹å¾´é‡ãƒªã‚¹ãƒˆä¿å­˜å®Œäº†: {feature_list_path}")

# ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–ç”¨ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚‚ä¿å­˜ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
if selected_model in ["xgboost", "lightgbm"]:
    # Tree-based modelsã¯æ­£è¦åŒ–ä¸è¦
    scaler = None
else:
    # ä»–ã®ãƒ¢ãƒ‡ãƒ«ç”¨ã«æ­£è¦åŒ–
    scaler = StandardScaler()
    scaler.fit(X_train)
    
if scaler is not None:
    scaler_path = os.path.join(model_dir, "scaler.pkl")
    with open(scaler_path, 'wb') as f:
        pickle.dump(scaler, f)
    print(f"âœ… ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ä¿å­˜å®Œäº†: {scaler_path}")# ã‚»ãƒ« 6: ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ & ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º

# 4. å­¦ç¿’ & ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
print("ğŸ¤– ãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹...")

selected_model = model_widget.value

# ãƒ¢ãƒ‡ãƒ«å®šç¾©ã¨å­¦ç¿’
if selected_model == "xgboost":
    print("ğŸš€ XGBoost ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ä¸­...")
    model = xgb.XGBClassifier(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        random_state=42
    )
    model.fit(X_train, y_train)
    
elif selected_model == "lightgbm":
    print("âš¡ LightGBM ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ä¸­...")
    model = lgb.LGBMClassifier(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        random_state=42,
        verbose=-1
    )
    model.fit(X_train, y_train)
    
elif selected_model == "rl":
    print("ğŸ§  ç°¡æ˜“RL (ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ) ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ä¸­...")
    from sklearn.ensemble import RandomForestClassifier
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        random_state=42
    )
    model.fit(X_train, y_train)

# äºˆæ¸¬å®Ÿè¡Œ
print("ğŸ”® äºˆæ¸¬ã‚’å®Ÿè¡Œä¸­...")
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred_proba)

print("âœ… å­¦ç¿’å®Œäº†ï¼")

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹çµæœã‚’ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
print("\nğŸ“Š å­¦ç¿’çµæœãƒ¡ãƒˆãƒªã‚¯ã‚¹:")
metrics_df = pd.DataFrame({
    'ãƒ¡ãƒˆãƒªã‚¯ã‚¹': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC'],
    'ã‚¹ã‚³ã‚¢': [accuracy, precision, recall, f1, auc],
    'èª¬æ˜': [
        'å…¨ä½“ã®æ­£è§£ç‡',
        'äºˆæ¸¬ã—ãŸä¸Šæ˜‡ã®ã†ã¡å®Ÿéš›ã«ä¸Šæ˜‡ã—ãŸå‰²åˆ',
        'å®Ÿéš›ã«ä¸Šæ˜‡ã—ãŸã†ã¡äºˆæ¸¬ã§ããŸå‰²åˆ',
        'Precision ã¨ Recall ã®èª¿å’Œå¹³å‡',
        'ROCæ›²ç·šã®ä¸‹å´é¢ç©'
    ]
})

# ã‚¹ã‚³ã‚¢ã‚’3æ¡ã§è¡¨ç¤º
metrics_df['ã‚¹ã‚³ã‚¢'] = metrics_df['ã‚¹ã‚³ã‚¢'].round(3)

display(metrics_df)

# åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ
print("\nğŸ“‹ è©³ç´°åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
report = classification_report(y_test, y_pred, output_dict=True)
report_df = pd.DataFrame(report).transpose()
display(report_df.round(3))# ã‚»ãƒ« 5: ãƒ‡ãƒ¼ã‚¿æº–å‚™ & ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆ

# 4. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®ä½œæˆ
print("ğŸ¯ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã‚’ä½œæˆä¸­...")

# ä¾¡æ ¼å¤‰å‹•æ–¹å‘ã‚’äºˆæ¸¬ï¼ˆä¸Šæ˜‡=1, ä¸‹é™=0ï¼‰
prediction_days = target_days_widget.value
df_ml = df_features.copy()

# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆï¼ˆæŒ‡å®šæ—¥å¾Œã®ä¾¡æ ¼ãŒç¾åœ¨ã‚ˆã‚Šé«˜ã„ã‹ã©ã†ã‹ï¼‰
df_ml['target'] = (df_ml['Close'].shift(-prediction_days) > df_ml['Close']).astype(int)

# æœ€å¾Œã®Næ—¥åˆ†ã¯äºˆæ¸¬ã§ããªã„ã®ã§å‰Šé™¤
df_ml = df_ml[:-prediction_days]

print(f"âœ… ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ä½œæˆå®Œäº†")
print(f"ğŸ“Š ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ:")
target_counts = df_ml['target'].value_counts()
print(f"  - ä¸‹é™ (0): {target_counts[0]} ä»¶ ({target_counts[0]/len(df_ml)*100:.1f}%)")
print(f"  - ä¸Šæ˜‡ (1): {target_counts[1]} ä»¶ ({target_counts[1]/len(df_ml)*100:.1f}%)")

# ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†é›¢
feature_columns = [col for col in df_ml.columns if col != 'target']
X = df_ml[feature_columns]
y = df_ml['target']

print(f"\nğŸ“Š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æ¦‚è¦:")
print(f"  - ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X)}")
print(f"  - ç‰¹å¾´é‡æ•°: {len(feature_columns)}")
print(f"  - æ­£ä¾‹ç‡: {y.mean():.3f}")

# è¨“ç·´/ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²
test_size = test_size_widget.value
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=test_size, 
    random_state=42, 
    stratify=y
)

print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²çµæœ:")
print(f"  - è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X_train.shape}")
print(f"  - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test.shape}")
print(f"  - è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ­£ä¾‹ç‡: {y_train.mean():.3f}")
print(f"  - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ­£ä¾‹ç‡: {y_test.mean():.3f}")# ã‚»ãƒ« 4: ãƒ¢ãƒ‡ãƒ«é¸æŠ UI

# 3. ãƒ¢ãƒ‡ãƒ«é¸æŠ UI
import ipywidgets as widgets
from IPython.display import display

print("ğŸ›ï¸ ãƒ¢ãƒ‡ãƒ«é¸æŠ UI")

# ãƒ¢ãƒ‡ãƒ«é¸æŠç”¨ã®ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³
model_widget = widgets.Dropdown(
    options=[
        ("XGBoost", "xgboost"),
        ("LightGBM", "lightgbm"),
        ("Reinforcement Learning (ç°¡æ˜“ç‰ˆ)", "rl")
    ],
    value="xgboost",
    description="ãƒ¢ãƒ‡ãƒ«:"
)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
test_size_widget = widgets.FloatSlider(
    value=0.2,
    min=0.1,
    max=0.4,
    step=0.05,
    description="ãƒ†ã‚¹ãƒˆã‚µã‚¤ã‚º:"
)

target_days_widget = widgets.IntSlider(
    value=1,
    min=1,
    max=5,
    step=1,
    description="äºˆæ¸¬æœŸé–“ (æ—¥):"
)

# ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã‚’è¡¨ç¤º
print("ğŸ“Š å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šã—ã¦ãã ã•ã„:")
display(model_widget)
display(test_size_widget)
display(target_days_widget)

# ç¾åœ¨ã®é¸æŠã‚’è¡¨ç¤ºã™ã‚‹é–¢æ•°
def show_current_selection(change=None):
    print(f"\nğŸ¯ ç¾åœ¨ã®é¸æŠ:")
    print(f"  - ãƒ¢ãƒ‡ãƒ«: {model_widget.label}")
    print(f"  - ãƒ†ã‚¹ãƒˆã‚µã‚¤ã‚º: {test_size_widget.value}")
    print(f"  - äºˆæ¸¬æœŸé–“: {target_days_widget.value}æ—¥å¾Œ")

# é¸æŠå¤‰æ›´æ™‚ã«è¡¨ç¤ºã‚’æ›´æ–°
model_widget.observe(show_current_selection, names='value')
test_size_widget.observe(show_current_selection, names='value')
target_days_widget.observe(show_current_selection, names='value')

# åˆæœŸé¸æŠã‚’è¡¨ç¤º
show_current_selection()# ã‚»ãƒ« 3: ç‰¹å¾´é‡ç”Ÿæˆ

# 2. ç‰¹å¾´é‡ç”Ÿæˆ
print("ğŸ”§ ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã‚’ç”Ÿæˆä¸­...")

try:
    # utils.indicators.add_all(df) ã§ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã‚’è¿½åŠ 
    df_features = add_all(df_raw.copy())
    
    print(f"âœ… ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†: {df_features.shape}")
    print(f"ğŸ“Š è¿½åŠ ã•ã‚ŒãŸç‰¹å¾´é‡æ•°: {len(df_features.columns) - len(df_raw.columns)}")
    
    # è¿½åŠ ã•ã‚ŒãŸç‰¹å¾´é‡ã®ç¢ºèª
    new_features = [col for col in df_features.columns if col not in df_raw.columns]
    print(f"\nğŸ¯ æ–°ã—ã„ç‰¹å¾´é‡:")
    for i, feature in enumerate(new_features[:10]):  # æœ€åˆã®10å€‹ã‚’è¡¨ç¤º
        print(f"  {i+1}. {feature}")
    if len(new_features) > 10:
        print(f"  ... ä»– {len(new_features) - 10} å€‹")
    
    # æ¬ æå€¤ã®ç¢ºèªã¨å‡¦ç†
    print(f"\nğŸ” æ¬ æå€¤å‡¦ç†å‰: {df_features.isnull().sum().sum()} å€‹")
    df_features = df_features.dropna()
    print(f"âœ… æ¬ æå€¤å‡¦ç†å¾Œ: {df_features.shape}")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    print(f"\nğŸ“‹ ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:")
    display(df_features.head())
    
except Exception as e:
    print(f"âŒ ç‰¹å¾´é‡ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    df_features = df_raw.copy()# ã‚»ãƒ« 2: ãƒ‡ãƒ¼ã‚¿èª­è¾¼ & ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆ

# å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import pandas as pd
import numpy as np
import json
import os
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score, precision_score, recall_score
from sklearn.preprocessing import StandardScaler
import lightgbm as lgb
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå°‚ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from utils.drive_io import load_raw, save_data, get_project_path
from utils.indicators import add_all

print("âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†ï¼")

# 1. ãƒ‡ãƒ¼ã‚¿èª­è¾¼
print("\nğŸ“¥ ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’èª­è¾¼ä¸­...")

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ (01_fetch_data.ipynb ã§å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿)
try:
    df_raw = load_raw(symbol="SOL_USDT", timeframe="1d", limit=1000)
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­è¾¼å®Œäº†: {df_raw.shape}")
    print(f"ğŸ“‹ ãƒ‡ãƒ¼ã‚¿æ¦‚è¦:")
    display(df_raw.head())
except Exception as e:
    print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­è¾¼ã‚¨ãƒ©ãƒ¼: {e}")
    print("ğŸ’¡ 01_fetch_data.ipynb ã§ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ãã ã•ã„")# ã‚»ãƒ« 1: Google Drive ãƒã‚¦ãƒ³ãƒˆ & å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

# Google Drive ã‚’ãƒã‚¦ãƒ³ãƒˆ
from google.colab import drive
drive.mount("/content/drive")

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
!pip install scikit-learn lightgbm xgboost tensorflow pandas_ta pyti ta
!pip install ipywidgets

# ãƒ‘ã‚¹ã®è¨­å®š
import sys
sys.path.append("/content/drive/MyDrive/kucoin_bot")

print("âœ… ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†ï¼")# ğŸ§  ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ (Issue #2)

**ç›®çš„**: æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹è©•ä¾¡ã‚’è¡Œã†

## å®Ÿè¡Œãƒ•ãƒ­ãƒ¼
1. **ãƒ‡ãƒ¼ã‚¿èª­è¾¼** - `utils/drive_io.load_raw()` ã§ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
2. **ç‰¹å¾´é‡ç”Ÿæˆ** - `utils.indicators.add_all(df)` ã§ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã‚’è¿½åŠ 
3. **ãƒ¢ãƒ‡ãƒ«é¸æŠ UI** - widgets ã§ XGBoost / LightGBM / RL ã‚’ãƒ—ãƒ«ãƒ€ã‚¦ãƒ³é¸æŠ
4. **å­¦ç¿’ & ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º** - F1, AUC ãªã©ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
5. **ãƒ¢ãƒ‡ãƒ«ä¿å­˜** - GDrive `/data/models/{date}/model.pkl` ã«ä¿å­˜
6. **ãƒ¡ã‚¿æƒ…å ± JSON å‡ºåŠ›** - æ¬¡å·¥ç¨‹ï¼ˆãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆï¼‰ç”¨ã®æƒ…å ±ã‚’å‡ºåŠ›

## å—ã‘å…¥ã‚Œæ¡ä»¶
âœ… Colab ä¸Šã§ãƒ©ãƒ³ã‚¿ã‚¤ãƒ å†èµ·å‹•å¾Œã§ã‚‚ 1 â†’ 6 ãŒå†ç¾å¯èƒ½  
âœ… ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆF1, AUC ãªã©ï¼‰ãŒã‚»ãƒ«ã«ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤ºã•ã‚Œã‚‹