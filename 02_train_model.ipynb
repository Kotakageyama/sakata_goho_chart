# セル 8: メタ情報 JSON 出力

# 6. 次工程（バックテスト）へ引き継ぐためのメタ情報 JSON 出力
print("📋 メタ情報を作成中...")

# メタ情報の作成
metadata = {
    "model_info": {
        "model_type": selected_model,
        "model_path": model_path,
        "feature_list_path": feature_list_path,
        "scaler_path": scaler_path if scaler is not None else None,
        "creation_date": today,
        "creation_datetime": datetime.now().isoformat()
    },
    "data_info": {
        "symbol": "SOL_USDT",
        "timeframe": "1d",
        "prediction_days": prediction_days,
        "total_samples": len(df_ml),
        "train_samples": len(X_train),
        "test_samples": len(X_test),
        "feature_count": len(feature_columns),
        "target_distribution": {
            "down_count": int(target_counts[0]),
            "up_count": int(target_counts[1]),
            "positive_rate": float(y.mean())
        }
    },
    "model_performance": {
        "accuracy": float(accuracy),
        "precision": float(precision),
        "recall": float(recall),
        "f1_score": float(f1),
        "auc": float(auc)
    },
    "training_parameters": {
        "test_size": float(test_size),
        "random_state": 42
    },
    "feature_categories": {
        "basic_ohlcv": [col for col in feature_columns if col in ['Open', 'High', 'Low', 'Close', 'Volume']],
        "technical_indicators": [col for col in feature_columns if any(indicator in col for indicator in ['SMA', 'RSI', 'MACD', 'ATR', 'bb_'])],
        "lag_features": [col for col in feature_columns if 'lag' in col],
        "pattern_features": [col for col in feature_columns if col in ['doji', 'hammer', 'shooting_star', 'engulfing_bullish', 'engulfing_bearish', 'three_white_soldiers', 'three_black_crows']]
    }
}

# メタ情報をJSONファイルに保存
metadata_path = os.path.join(model_dir, "metadata.json")
with open(metadata_path, 'w', encoding='utf-8') as f:
    json.dump(metadata, f, ensure_ascii=False, indent=2)

print(f"✅ メタ情報保存完了: {metadata_path}")

# 結果サマリーの表示
print(f"\n🎉 トレーニング完了サマリー:")
print(f"  - モデル: {selected_model.upper()}")
print(f"  - 精度: {accuracy:.3f}")
print(f"  - F1スコア: {f1:.3f}")
print(f"  - AUC: {auc:.3f}")
print(f"  - 保存先: {model_dir}")
print(f"  - 次のステップ: 03_backtest.ipynb でバックテストを実行してください")

# メタ情報の内容を表示
print(f"\n📋 保存されたメタ情報:")
display(pd.DataFrame([
    {"項目": "モデルタイプ", "値": metadata["model_info"]["model_type"]},
    {"項目": "予測期間", "値": f"{metadata['data_info']['prediction_days']}日後"},
    {"項目": "サンプル数", "値": metadata["data_info"]["total_samples"]},
    {"項目": "特徴量数", "値": metadata["data_info"]["feature_count"]},
    {"項目": "正例率", "値": f"{metadata['data_info']['target_distribution']['positive_rate']:.3f}"},
    {"項目": "F1スコア", "値": f"{metadata['model_performance']['f1_score']:.3f}"},
    {"項目": "AUC", "値": f"{metadata['model_performance']['auc']:.3f}"}
]))# セル 7: モデル保存

# 5. 学習済みモデルを GDrive /data/models/{date}/model.pkl に保存
print("💾 モデルを保存中...")

# 日付フォルダの作成
today = datetime.now().strftime("%Y%m%d")
project_path = get_project_path()
model_dir = os.path.join(project_path, "data", "models", today)
os.makedirs(model_dir, exist_ok=True)

# モデル保存パス
model_path = os.path.join(model_dir, "model.pkl")

# モデル保存
import pickle
with open(model_path, 'wb') as f:
    pickle.dump(model, f)

print(f"✅ モデル保存完了: {model_path}")

# 特徴量リストも保存
feature_list_path = os.path.join(model_dir, "features.json")
with open(feature_list_path, 'w', encoding='utf-8') as f:
    json.dump(feature_columns, f, ensure_ascii=False, indent=2)

print(f"✅ 特徴量リスト保存完了: {feature_list_path}")

# データ正規化用のスケーラーも保存（必要に応じて）
if selected_model in ["xgboost", "lightgbm"]:
    # Tree-based modelsは正規化不要
    scaler = None
else:
    # 他のモデル用に正規化
    scaler = StandardScaler()
    scaler.fit(X_train)
    
if scaler is not None:
    scaler_path = os.path.join(model_dir, "scaler.pkl")
    with open(scaler_path, 'wb') as f:
        pickle.dump(scaler, f)
    print(f"✅ スケーラー保存完了: {scaler_path}")# セル 6: モデル学習 & メトリクス表示

# 4. 学習 & メトリクス表示
print("🤖 モデル学習開始...")

selected_model = model_widget.value

# モデル定義と学習
if selected_model == "xgboost":
    print("🚀 XGBoost モデルを学習中...")
    model = xgb.XGBClassifier(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        random_state=42
    )
    model.fit(X_train, y_train)
    
elif selected_model == "lightgbm":
    print("⚡ LightGBM モデルを学習中...")
    model = lgb.LGBMClassifier(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        random_state=42,
        verbose=-1
    )
    model.fit(X_train, y_train)
    
elif selected_model == "rl":
    print("🧠 簡易RL (ランダムフォレスト) モデルを学習中...")
    from sklearn.ensemble import RandomForestClassifier
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        random_state=42
    )
    model.fit(X_train, y_train)

# 予測実行
print("🔮 予測を実行中...")
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]

# メトリクス計算
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred_proba)

print("✅ 学習完了！")

# メトリクス結果をテーブル表示
print("\n📊 学習結果メトリクス:")
metrics_df = pd.DataFrame({
    'メトリクス': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC'],
    'スコア': [accuracy, precision, recall, f1, auc],
    '説明': [
        '全体の正解率',
        '予測した上昇のうち実際に上昇した割合',
        '実際に上昇したうち予測できた割合',
        'Precision と Recall の調和平均',
        'ROC曲線の下側面積'
    ]
})

# スコアを3桁で表示
metrics_df['スコア'] = metrics_df['スコア'].round(3)

display(metrics_df)

# 分類レポート
print("\n📋 詳細分類レポート:")
report = classification_report(y_test, y_pred, output_dict=True)
report_df = pd.DataFrame(report).transpose()
display(report_df.round(3))# セル 5: データ準備 & ターゲット作成

# 4. ターゲット変数の作成
print("🎯 ターゲット変数を作成中...")

# 価格変動方向を予測（上昇=1, 下降=0）
prediction_days = target_days_widget.value
df_ml = df_features.copy()

# ターゲット作成（指定日後の価格が現在より高いかどうか）
df_ml['target'] = (df_ml['Close'].shift(-prediction_days) > df_ml['Close']).astype(int)

# 最後のN日分は予測できないので削除
df_ml = df_ml[:-prediction_days]

print(f"✅ ターゲット変数作成完了")
print(f"📊 ターゲット分布:")
target_counts = df_ml['target'].value_counts()
print(f"  - 下降 (0): {target_counts[0]} 件 ({target_counts[0]/len(df_ml)*100:.1f}%)")
print(f"  - 上昇 (1): {target_counts[1]} 件 ({target_counts[1]/len(df_ml)*100:.1f}%)")

# 特徴量とターゲットの分離
feature_columns = [col for col in df_ml.columns if col != 'target']
X = df_ml[feature_columns]
y = df_ml['target']

print(f"\n📊 学習データ概要:")
print(f"  - サンプル数: {len(X)}")
print(f"  - 特徴量数: {len(feature_columns)}")
print(f"  - 正例率: {y.mean():.3f}")

# 訓練/テストデータの分割
test_size = test_size_widget.value
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=test_size, 
    random_state=42, 
    stratify=y
)

print(f"\n📊 データ分割結果:")
print(f"  - 訓練データ: {X_train.shape}")
print(f"  - テストデータ: {X_test.shape}")
print(f"  - 訓練データ正例率: {y_train.mean():.3f}")
print(f"  - テストデータ正例率: {y_test.mean():.3f}")# セル 4: モデル選択 UI

# 3. モデル選択 UI
import ipywidgets as widgets
from IPython.display import display

print("🎛️ モデル選択 UI")

# モデル選択用のドロップダウン
model_widget = widgets.Dropdown(
    options=[
        ("XGBoost", "xgboost"),
        ("LightGBM", "lightgbm"),
        ("Reinforcement Learning (簡易版)", "rl")
    ],
    value="xgboost",
    description="モデル:"
)

# パラメータ設定
test_size_widget = widgets.FloatSlider(
    value=0.2,
    min=0.1,
    max=0.4,
    step=0.05,
    description="テストサイズ:"
)

target_days_widget = widgets.IntSlider(
    value=1,
    min=1,
    max=5,
    step=1,
    description="予測期間 (日):"
)

# ウィジェットを表示
print("📊 学習パラメータを設定してください:")
display(model_widget)
display(test_size_widget)
display(target_days_widget)

# 現在の選択を表示する関数
def show_current_selection(change=None):
    print(f"\n🎯 現在の選択:")
    print(f"  - モデル: {model_widget.label}")
    print(f"  - テストサイズ: {test_size_widget.value}")
    print(f"  - 予測期間: {target_days_widget.value}日後")

# 選択変更時に表示を更新
model_widget.observe(show_current_selection, names='value')
test_size_widget.observe(show_current_selection, names='value')
target_days_widget.observe(show_current_selection, names='value')

# 初期選択を表示
show_current_selection()# セル 3: 特徴量生成

# 2. 特徴量生成
print("🔧 テクニカル指標を生成中...")

try:
    # utils.indicators.add_all(df) でテクニカル指標を追加
    df_features = add_all(df_raw.copy())
    
    print(f"✅ 特徴量生成完了: {df_features.shape}")
    print(f"📊 追加された特徴量数: {len(df_features.columns) - len(df_raw.columns)}")
    
    # 追加された特徴量の確認
    new_features = [col for col in df_features.columns if col not in df_raw.columns]
    print(f"\n🎯 新しい特徴量:")
    for i, feature in enumerate(new_features[:10]):  # 最初の10個を表示
        print(f"  {i+1}. {feature}")
    if len(new_features) > 10:
        print(f"  ... 他 {len(new_features) - 10} 個")
    
    # 欠損値の確認と処理
    print(f"\n🔍 欠損値処理前: {df_features.isnull().sum().sum()} 個")
    df_features = df_features.dropna()
    print(f"✅ 欠損値処理後: {df_features.shape}")
    
    # データプレビュー
    print(f"\n📋 特徴量データプレビュー:")
    display(df_features.head())
    
except Exception as e:
    print(f"❌ 特徴量生成エラー: {e}")
    df_features = df_raw.copy()# セル 2: データ読込 & ライブラリインポート

# 必要ライブラリのインポート
import pandas as pd
import numpy as np
import json
import os
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score, precision_score, recall_score
from sklearn.preprocessing import StandardScaler
import lightgbm as lgb
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

# プロジェクト専用ライブラリ
from utils.drive_io import load_raw, save_data, get_project_path
from utils.indicators import add_all

print("✅ ライブラリインポート完了！")

# 1. データ読込
print("\n📥 生データを読込中...")

# デフォルトのデータを読み込み (01_fetch_data.ipynb で取得したデータ)
try:
    df_raw = load_raw(symbol="SOL_USDT", timeframe="1d", limit=1000)
    print(f"✅ データ読込完了: {df_raw.shape}")
    print(f"📋 データ概要:")
    display(df_raw.head())
except Exception as e:
    print(f"❌ データ読込エラー: {e}")
    print("💡 01_fetch_data.ipynb でデータを取得してください")# セル 1: Google Drive マウント & 必要ライブラリのインストール

# Google Drive をマウント
from google.colab import drive
drive.mount("/content/drive")

# 必要なライブラリをインストール
!pip install scikit-learn lightgbm xgboost tensorflow pandas_ta pyti ta
!pip install ipywidgets

# パスの設定
import sys
sys.path.append("/content/drive/MyDrive/kucoin_bot")

print("✅ セットアップ完了！")# 🧠 トレーニングノートブック (Issue #2)

**目的**: 機械学習モデルの学習とメトリクス評価を行う

## 実行フロー
1. **データ読込** - `utils/drive_io.load_raw()` で生データを取得
2. **特徴量生成** - `utils.indicators.add_all(df)` でテクニカル指標を追加
3. **モデル選択 UI** - widgets で XGBoost / LightGBM / RL をプルダウン選択
4. **学習 & メトリクス表示** - F1, AUC などをテーブル表示
5. **モデル保存** - GDrive `/data/models/{date}/model.pkl` に保存
6. **メタ情報 JSON 出力** - 次工程（バックテスト）用の情報を出力

## 受け入れ条件
✅ Colab 上でランタイム再起動後でも 1 → 6 が再現可能  
✅ メトリクス（F1, AUC など）がセルにテーブル表示される