# ðŸ† ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ & æœ€çµ‚ä¿å­˜
import pickle
import os

print("ðŸ”® Creating Ultimate Meta-Model...")

# ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
if len(top_2_model_names) >= 2:
    print(f"ðŸŽ¯ Building ensemble with top models: {top_2_model_names}")
    
    # ãƒˆãƒƒãƒ—2ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
    top_models = [trained_models[name] for name in top_2_model_names]
    
    try:
        # Stacking Model ã‚’ä½œæˆ
        from src.advanced_models import create_stacked_model
        meta_model = create_stacked_model(top_models)
        
        print("ðŸ”¥ Training Stacked Meta-Model...")
        if cv_available:
            meta_model.fit(X, y)
            final_pred = meta_model.predict(X)
            final_pred_proba = meta_model.predict_proba(X)
            final_metrics = calculate_extended_metrics(y, final_pred, final_pred_proba)
        else:
            meta_model.fit(X_train, y_train)
            final_pred = meta_model.predict(X_test)
            final_pred_proba = meta_model.predict_proba(X_test)
            final_metrics = calculate_extended_metrics(y_test, final_pred, final_pred_proba)
        
        final_model = meta_model
        final_model_name = "stacked_meta_model"
        
    except ImportError:
        print("âš ï¸ Stacking not available - using ensemble averaging")
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å¹³å‡
        if cv_available:
            all_probas = []
            for model_name in top_2_model_names:
                model = trained_models[model_name]
                proba = model.predict_proba(X)
                all_probas.append(proba[:, 1])
            
            ensemble_proba = np.mean(all_probas, axis=0)
            final_pred = (ensemble_proba > 0.5).astype(int)
            final_pred_proba = np.column_stack([1 - ensemble_proba, ensemble_proba])
            final_metrics = calculate_extended_metrics(y, final_pred, final_pred_proba)
        else:
            all_probas = []
            for model_name in top_2_model_names:
                model = trained_models[model_name]
                proba = model.predict_proba(X_test)
                all_probas.append(proba[:, 1])
            
            ensemble_proba = np.mean(all_probas, axis=0)
            final_pred = (ensemble_proba > 0.5).astype(int)
            final_pred_proba = np.column_stack([1 - ensemble_proba, ensemble_proba])
            final_metrics = calculate_extended_metrics(y_test, final_pred, final_pred_proba)
        
        final_model = trained_models[top_2_model_names[0]]  # æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã‚’ä»£è¡¨ã¨ã—ã¦ä¿å­˜
        final_model_name = "ensemble_meta_model"

else:
    print("âš ï¸ Using single best model as final model")
    if len(valid_results) > 0:
        best_model_name = valid_results.sort_values('auc', ascending=False).index[0]
        final_model = trained_models[best_model_name]
        final_model_name = best_model_name
        final_metrics = model_results[best_model_name]
    else:
        print("âŒ No valid models available")
        final_model = None
        final_model_name = None
        final_metrics = {}

# æœ€çµ‚çµæžœã®è¡¨ç¤º
if final_model is not None:
    print(f"\n{'='*80}")
    print("ðŸŽ‰ FINAL WORLD-CLASS MODEL RESULTS")
    print(f"{'='*80}")
    print(f"Model: {final_model_name}")
    print(f"AUC: {final_metrics.get('auc', 0):.4f}")
    print(f"F1 Score: {final_metrics.get('f1_score', 0):.4f}")
    print(f"Sharpe-like: {final_metrics.get('sharpe_like', 0):.4f}")
    print(f"Max Drawdown: {final_metrics.get('max_drawdown_approx', 0):.4f}")
    print(f"Calmar Ratio: {final_metrics.get('calmar_approx', 0):.4f}")
    
    # ä¸–ç•Œã‚¯ãƒ©ã‚¹åŸºæº–ã®æœ€çµ‚ãƒã‚§ãƒƒã‚¯
    auc = final_metrics.get('auc', 0)
    sharpe_like = final_metrics.get('sharpe_like', 0)
    max_dd = final_metrics.get('max_drawdown_approx', 0)
    
    print(f"\nðŸŽ¯ World-Class Criteria Achievement:")
    print(f"AUC > 0.65: {'âœ…' if auc > 0.65 else 'âŒ'} ({auc:.4f})")
    print(f"Sharpe > 1.5: {'âœ…' if sharpe_like > 1.5 else 'âŒ'} ({sharpe_like:.4f})")
    print(f"MaxDD > -10%: {'âœ…' if max_dd > -0.1 else 'âŒ'} ({max_dd:.4f})")
    
    # ç›®æ¨™é”æˆåº¦
    total_score = sum([auc > 0.65, sharpe_like > 1.5, max_dd > -0.1])
    if total_score >= 2:
        print(f"\nðŸ† WORLD-CLASS CRITERIA MET! Score: {total_score}/3")
    else:
        print(f"\nâš ï¸ Criteria partially met. Score: {total_score}/3")

# ãƒ¢ãƒ‡ãƒ«ä¿å­˜
print(f"\nðŸ’¾ Saving models...")

today = datetime.now().strftime("%Y%m%d")
model_dir = f"data/models/{today}"
os.makedirs(model_dir, exist_ok=True)

if final_model is not None:
    # ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    model_path = os.path.join(model_dir, "model.pkl")
    with open(model_path, 'wb') as f:
        pickle.dump(final_model, f)
    print(f"âœ… Main model saved: {model_path}")
    
    # ç‰¹å¾´é‡ãƒªã‚¹ãƒˆä¿å­˜
    features_path = os.path.join(model_dir, "features.json")
    with open(features_path, 'w', encoding='utf-8') as f:
        json.dump(feature_columns, f, ensure_ascii=False, indent=2)
    print(f"âœ… Features saved: {features_path}")
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    metadata = {
        "model_info": {
            "model_type": final_model_name,
            "model_path": model_path,
            "feature_list_path": features_path,
            "creation_date": today,
            "creation_datetime": datetime.now().isoformat(),
            "top_models_used": top_2_model_names if len(top_2_model_names) >= 2 else [final_model_name]
        },
        "data_info": {
            "symbol": "SOL_USDT",
            "timeframe": "1d",
            "prediction_days": prediction_days,
            "total_samples": len(df_ml),
            "feature_count": len(feature_columns),
            "target_distribution": {
                "class_0_count": int(target_counts[0]),
                "class_1_count": int(target_counts[1]),
                "positive_rate": float(y.mean())
            }
        },
        "model_performance": final_metrics,
        "world_class_criteria": {
            "auc_threshold": 0.65,
            "sharpe_threshold": 1.5,
            "max_dd_threshold": -0.1,
            "criteria_met": total_score if final_model else 0,
            "total_criteria": 3
        },
        "training_parameters": {
            "purged_cv_used": cv_available,
            "cv_splits": purged_cv.n_splits if cv_available else None,
            "advanced_features_used": True,
            "ensemble_method": "stacking" if len(top_2_model_names) >= 2 else "single_model"
        },
        "experiment_tracking": {
            "mlflow_run_id": run_id,
            "git_commit": git_commit if 'git_commit' in locals() else "unknown"
        }
    }
    
    metadata_path = os.path.join(model_dir, "metadata.json")
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    print(f"âœ… Metadata saved: {metadata_path}")
    
    # ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«çµæžœã‚’ä¿å­˜
    all_results_path = os.path.join(model_dir, "all_model_results.json")
    with open(all_results_path, 'w', encoding='utf-8') as f:
        # numpyåž‹ã‚’é€šå¸¸ã®Pythonåž‹ã«å¤‰æ›
        serializable_results = {}
        for k, v in model_results.items():
            if isinstance(v, dict):
                serializable_results[k] = {
                    key: float(val) if isinstance(val, (np.floating, np.integer)) else val 
                    for key, val in v.items()
                }
            else:
                serializable_results[k] = v
        json.dump(serializable_results, f, ensure_ascii=False, indent=2)
    print(f"âœ… All results saved: {all_results_path}")

# æœ€çµ‚ã‚µãƒžãƒªãƒ¼
print(f"\n{'='*80}")
print("ðŸŽ¯ WORLD-CLASS MODEL TRAINING COMPLETE!")
print(f"{'='*80}")
print(f"ðŸ“‚ Models saved in: {model_dir}")
print(f"ðŸŽ¯ Final model: {final_model_name}")
print(f"âœ… Next step: Run 03_backtest.ipynb for comprehensive backtesting")

if run_id:
    print(f"ðŸ“Š MLflow run ID: {run_id}")
    print(f"ðŸ”— View experiments in MLflow UI")

print(f"\nðŸš€ Ready to achieve Sharpe > 3 and MaxDD < 5% in backtesting!")
print("ðŸ† World-class model development completed successfully! ðŸ†")# ðŸš€ ä¸–ç•Œã‚¯ãƒ©ã‚¹ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ & è©•ä¾¡
import lightgbm as lgb
import xgboost as xgb
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report
import optuna
from datetime import datetime
import json

# æ‹¡å¼µãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—é–¢æ•°
def calculate_extended_metrics(y_true, y_pred, y_pred_proba):
    """ä¸–ç•Œã‚¯ãƒ©ã‚¹ãƒ¢ãƒ‡ãƒ«ç”¨ã®æ‹¡å¼µãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
    metrics = {
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),
        'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),
        'f1_score': f1_score(y_true, y_pred, average='weighted', zero_division=0),
        'auc': roc_auc_score(y_true, y_pred_proba[:, 1]) if y_pred_proba.shape[1] > 1 else 0.5
    }
    
    # é‡‘èžç‰¹åŒ–ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    returns = y_pred_proba[:, 1] - 0.5  # äºˆæ¸¬ç¢ºçŽ‡ã‹ã‚‰æœŸå¾…ãƒªã‚¿ãƒ¼ãƒ³
    if len(returns) > 1:
        sharpe_like = np.mean(returns) / (np.std(returns) + 1e-8) * np.sqrt(252)
        metrics['sharpe_like'] = sharpe_like
        
        # æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³è¿‘ä¼¼
        cum_returns = np.cumsum(returns)
        running_max = np.maximum.accumulate(cum_returns)
        drawdown = (cum_returns - running_max)
        max_dd = np.min(drawdown)
        metrics['max_drawdown_approx'] = max_dd
        
        # Calmar ratioè¿‘ä¼¼
        annual_return = np.mean(returns) * 252
        calmar = annual_return / (abs(max_dd) + 1e-8)
        metrics['calmar_approx'] = calmar
    
    return metrics

# ä¸–ç•Œã‚¯ãƒ©ã‚¹ãƒ¢ãƒ‡ãƒ«å€™è£œã®å®šç¾©
print("ðŸ† Initializing World-Class Models...")

# åŸºæœ¬ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ï¼‰
base_models = {
    'lgb_baseline': lgb.LGBMClassifier(
        n_estimators=300,
        max_depth=8,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        verbose=-1
    ),
    'xgb_baseline': xgb.XGBClassifier(
        n_estimators=300,
        max_depth=8,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        verbosity=0
    ),
    'rf_baseline': RandomForestClassifier(
        n_estimators=200,
        max_depth=12,
        random_state=42,
        n_jobs=-1
    )
}

# é«˜åº¦ãªãƒ¢ãƒ‡ãƒ«
try:
    from src.advanced_models import get_advanced_models, create_ensemble_model, create_stacked_model
    advanced_models = get_advanced_models()
    print("âœ… Advanced models loaded")
except ImportError:
    print("âš ï¸ Advanced models not available - using base models only")
    advanced_models = {}

# å…¨ãƒ¢ãƒ‡ãƒ«ã®çµåˆ
all_models = {**base_models, **advanced_models}

print(f"ðŸŽ¯ Total models to train: {len(all_models)}")

# ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ & è©•ä¾¡
print("\nðŸ”¥ Training world-class models...")
model_results = {}
trained_models = {}

for model_name, model in all_models.items():
    print(f"\n{'='*60}")
    print(f"ðŸš€ Training {model_name}...")
    print(f"{'='*60}")
    
    try:
        if cv_available:
            # Purged CV ã«ã‚ˆã‚‹è©•ä¾¡
            print("ðŸ“Š Purged Cross-Validation...")
            cv_scores = purged_cv_score(model, X, y, cv=purged_cv, scoring='roc_auc', verbose=1)
            
            cv_mean = np.mean(cv_scores)
            cv_std = np.std(cv_scores)
            
            print(f"âœ… CV AUC: {cv_mean:.4f} (+/- {cv_std:.4f})")
            
            # å…¨ãƒ‡ãƒ¼ã‚¿ã§ã®å­¦ç¿’
            print("ðŸŽ¯ Training on full dataset...")
            model.fit(X, y)
            
            # äºˆæ¸¬ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ï¼‰
            y_pred = model.predict(X)
            y_pred_proba = model.predict_proba(X)
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
            metrics = calculate_extended_metrics(y, y_pred, y_pred_proba)
            metrics['cv_auc_mean'] = cv_mean
            metrics['cv_auc_std'] = cv_std
            
        else:
            # æ¨™æº–çš„ãªè©•ä¾¡
            print("ðŸ“Š Standard train/test evaluation...")
            model.fit(X_train, y_train)
            
            # äºˆæ¸¬
            y_pred = model.predict(X_test)
            y_pred_proba = model.predict_proba(X_test)
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
            metrics = calculate_extended_metrics(y_test, y_pred, y_pred_proba)
            metrics['cv_auc_mean'] = metrics['auc']
            metrics['cv_auc_std'] = 0.0
        
        # çµæžœä¿å­˜
        model_results[model_name] = metrics
        trained_models[model_name] = model
        
        print(f"âœ… {model_name} Results:")
        print(f"   AUC: {metrics['auc']:.4f}")
        print(f"   F1:  {metrics['f1_score']:.4f}")
        print(f"   Sharpe-like: {metrics.get('sharpe_like', 0):.4f}")
        print(f"   Max DD: {metrics.get('max_drawdown_approx', 0):.4f}")
        print(f"   Calmar: {metrics.get('calmar_approx', 0):.4f}")
        
        # MLflowãƒ­ã‚°
        if run_id:
            try:
                with mlflow.start_run(run_id=run_id):
                    for metric_name, metric_value in metrics.items():
                        mlflow.log_metric(f"{model_name}_{metric_name}", metric_value)
            except:
                pass
        
    except Exception as e:
        print(f"âŒ Error training {model_name}: {str(e)}")
        model_results[model_name] = {'error': str(e)}

# çµæžœã‚µãƒžãƒªãƒ¼
print(f"\n{'='*80}")
print("ðŸ† WORLD-CLASS MODEL RESULTS SUMMARY")
print(f"{'='*80}")

# çµæžœã‚’DataFrameã«å¤‰æ›
results_df = pd.DataFrame(model_results).T
valid_results = results_df[~results_df.index.str.contains('error', na=False)]

if len(valid_results) > 0:
    # ãƒˆãƒƒãƒ—ãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®š
    valid_results = valid_results.select_dtypes(include=[np.number])
    
    print("\nðŸ“Š Model Performance Ranking:")
    print("-" * 50)
    
    # AUCã§ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    if 'auc' in valid_results.columns:
        top_models = valid_results.sort_values('auc', ascending=False)
        for i, (model_name, row) in enumerate(top_models.head(5).iterrows()):
            print(f"{i+1:2d}. {model_name:20s} - AUC: {row['auc']:.4f} | F1: {row['f1_score']:.4f}")
    
    # ä¸–ç•Œã‚¯ãƒ©ã‚¹åŸºæº–ã®ãƒã‚§ãƒƒã‚¯
    print(f"\nðŸŽ¯ World-Class Criteria Check:")
    print("-" * 40)
    
    world_class_candidates = []
    for model_name, metrics in model_results.items():
        if isinstance(metrics, dict) and 'error' not in metrics:
            auc = metrics.get('auc', 0)
            sharpe_like = metrics.get('sharpe_like', 0)
            max_dd = metrics.get('max_drawdown_approx', 0)
            
            world_class_score = 0
            if auc > 0.65: world_class_score += 1
            if sharpe_like > 1.5: world_class_score += 1
            if max_dd > -0.1: world_class_score += 1  # Less than 10% DD
            
            if world_class_score >= 2:
                world_class_candidates.append((model_name, world_class_score, metrics))
                print(f"âœ… {model_name}: Score {world_class_score}/3")
    
    # ãƒˆãƒƒãƒ—2ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠž
    if len(world_class_candidates) >= 2:
        world_class_candidates.sort(key=lambda x: x[1], reverse=True)
        top_2_models = world_class_candidates[:2]
        
        print(f"\nðŸ† TOP 2 WORLD-CLASS MODELS SELECTED:")
        for i, (model_name, score, metrics) in enumerate(top_2_models):
            print(f"{i+1}. {model_name} (Score: {score}/3)")
            print(f"   AUC: {metrics['auc']:.4f} | Sharpe: {metrics.get('sharpe_like', 0):.4f}")
        
        # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ç”¨ã«ãƒˆãƒƒãƒ—ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
        top_2_model_names = [name for name, _, _ in top_2_models]
        print(f"\nðŸŽ¯ Ready for ensemble/stacking: {top_2_model_names}")
        
    else:
        print("âš ï¸ No models met world-class criteria. Using best available models.")
        if len(valid_results) > 0:
            top_2_model_names = list(valid_results.sort_values('auc', ascending=False).head(2).index)
        else:
            top_2_model_names = []
    
    print(f"\nâœ… Model training complete! {len(valid_results)} models trained successfully.")
    
else:
    print("âŒ No models trained successfully.")
    top_2_model_names = []

print(f"\nðŸŽ¯ Next: Ensemble/Stacking with top models for ultimate performance!")# ðŸŽ¯ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆ & Purged Walk-Forward CV
from sklearn.model_selection import train_test_split

# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®ä½œæˆï¼ˆé«˜åº¦ãªæˆ¦ç•¥ï¼‰
print("ðŸŽ¯ Creating advanced target variables...")

# è¤‡æ•°ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæˆ¦ç•¥
target_strategies = {
    'price_direction': 3,  # 3æ—¥å¾Œã®ä¾¡æ ¼æ–¹å‘
    'volatility_adjusted': 5,  # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£èª¿æ•´æ¸ˆã¿ãƒªã‚¿ãƒ¼ãƒ³
    'risk_adjusted': 5,  # ãƒªã‚¹ã‚¯èª¿æ•´æ¸ˆã¿ãƒªã‚¿ãƒ¼ãƒ³
}

# ãƒ¡ã‚¤ãƒ³æˆ¦ç•¥: ãƒªã‚¹ã‚¯èª¿æ•´æ¸ˆã¿ãƒªã‚¿ãƒ¼ãƒ³
prediction_days = 5
df_ml = df_features.copy()

# ä¾¡æ ¼ãƒªã‚¿ãƒ¼ãƒ³ã®è¨ˆç®—
returns = df_ml['Close'].pct_change(prediction_days).shift(-prediction_days)
volatility = returns.rolling(window=20).std()

# ãƒªã‚¹ã‚¯èª¿æ•´æ¸ˆã¿ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆSharpe-likeï¼‰
risk_adjusted_returns = returns / (volatility + 1e-8)

# é«˜åº¦ãªã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: ä¸Šä½/ä¸‹ä½ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹
percentile_threshold = 0.3
upper_threshold = risk_adjusted_returns.quantile(1 - percentile_threshold)
lower_threshold = risk_adjusted_returns.quantile(percentile_threshold)

# 3ã‚¯ãƒ©ã‚¹åˆ†é¡žï¼ˆBuy=2, Hold=1, Sell=0ï¼‰ã‹ã‚‰ãƒã‚¤ãƒŠãƒªåˆ†é¡žã¸
df_ml['target_continuous'] = risk_adjusted_returns
df_ml['target_3class'] = 1  # Hold
df_ml.loc[risk_adjusted_returns > upper_threshold, 'target_3class'] = 2  # Buy
df_ml.loc[risk_adjusted_returns < lower_threshold, 'target_3class'] = 0  # Sell

# ãƒã‚¤ãƒŠãƒªåˆ†é¡žç”¨ï¼ˆBuy vs Othersï¼‰
df_ml['target'] = (df_ml['target_3class'] == 2).astype(int)

# æœ€å¾Œã®Næ—¥åˆ†ã‚’å‰Šé™¤ï¼ˆäºˆæ¸¬ä¸å¯èƒ½ï¼‰
df_ml = df_ml[:-prediction_days]

print(f"âœ… Target created with {prediction_days} day prediction horizon")
print(f"ðŸ“Š Target distribution:")
target_counts = df_ml['target'].value_counts()
print(f"  - Class 0 (Others): {target_counts[0]} ({target_counts[0]/len(df_ml)*100:.1f}%)")
print(f"  - Class 1 (Buy): {target_counts[1]} ({target_counts[1]/len(df_ml)*100:.1f}%)")

# ç‰¹å¾´é‡ã®é¸æŠž
feature_columns = [col for col in df_ml.columns if col not in ['target', 'target_3class', 'target_continuous']]
X = df_ml[feature_columns]
y = df_ml['target']

print(f"\nðŸ“Š Final ML dataset:")
print(f"  - Samples: {len(X)}")
print(f"  - Features: {len(feature_columns)}")
print(f"  - Positive class rate: {y.mean():.3f}")

# Purged Walk-Forward CV ã®è¨­å®š
print("\nðŸ”„ Setting up Purged Walk-Forward Cross-Validation...")
try:
    from src.purged_cv import PurgedWalkForwardCV, purged_cv_score
    
    # Purged CV ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é˜²æ­¢ï¼‰
    purged_cv = PurgedWalkForwardCV(
        n_splits=5,
        test_size=len(X) // 10,  # 10%ã‚’ãƒ†ã‚¹ãƒˆã‚µã‚¤ã‚ºã«
        purge_size=prediction_days,  # äºˆæ¸¬æœŸé–“åˆ†ã‚’ãƒ‘ãƒ¼ã‚¸
        embargo_size=2  # è¿½åŠ ã®ã‚¨ãƒ³ãƒãƒ¼ã‚´
    )
    
    print(f"âœ… Purged CV configured:")
    print(f"  - Splits: {purged_cv.n_splits}")
    print(f"  - Test size: {purged_cv.test_size}")
    print(f"  - Purge size: {purged_cv.purge_size}")
    print(f"  - Embargo size: {purged_cv.embargo_size}")
    
    # CVåˆ†å‰²ã®ç¢ºèª
    print(f"\nðŸ“‹ CV Split Overview:")
    for i, (train_idx, test_idx) in enumerate(purged_cv.split(X)):
        print(f"  Fold {i+1}: Train[{train_idx[0]:4d}:{train_idx[-1]:4d}] Test[{test_idx[0]:4d}:{test_idx[-1]:4d}]")
    
    cv_available = True
    
except ImportError:
    print("âš ï¸ Purged CV not available - using standard train/test split")
    cv_available = False
    
    # æ¨™æº–çš„ãªæ™‚ç³»åˆ—åˆ†å‰²
    split_date = df_ml.index[int(len(df_ml) * 0.8)]
    train_mask = df_ml.index <= split_date
    test_mask = df_ml.index > split_date
    
    X_train = X[train_mask]
    X_test = X[test_mask]
    y_train = y[train_mask]
    y_test = y[test_mask]
    
    print(f"âœ… Standard split:")
    print(f"  - Train: {len(X_train)} samples")
    print(f"  - Test: {len(X_test)} samples")

print("ðŸŽ¯ Data preparation for world-class modeling complete!")# ðŸ“Š ãƒ‡ãƒ¼ã‚¿èª­è¾¼ã¿ & é«˜åº¦ãªç‰¹å¾´é‡ç”Ÿæˆ
import pandas as pd
import numpy as np
from datetime import datetime
import subprocess
import git

# MLflowå®Ÿé¨“ç®¡ç†ã®è¨­å®š
try:
    import mlflow
    import mlflow.sklearn
    
    # Git commit hash ã‚’å–å¾—
    try:
        repo = git.Repo(search_parent_directories=True)
        git_commit = repo.head.object.hexsha
        print(f"ðŸ”— Git commit: {git_commit[:8]}")
    except:
        git_commit = "unknown"
    
    # MLflowå®Ÿé¨“é–‹å§‹
    experiment_name = f"world-class-model-{datetime.now().strftime('%Y%m%d')}"
    mlflow.set_experiment(experiment_name)
    
    with mlflow.start_run() as run:
        mlflow.set_tag("git_commit", git_commit)
        mlflow.set_tag("model_type", "world_class_ensemble")
        print(f"ðŸŽ¯ MLflow run started: {run.info.run_id}")
        run_id = run.info.run_id
    
except ImportError:
    print("âš ï¸ MLflow not available - skipping experiment tracking")
    run_id = None

# ãƒ‡ãƒ¼ã‚¿èª­è¾¼ã¿
print("ðŸ“¥ Loading data...")
try:
    # æ—¢å­˜ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’ä½¿ç”¨
    from src.utils import load_raw
    df_raw = load_raw(symbol="SOL_USDT", timeframe="1d", limit=2000)
    print(f"âœ… Raw data loaded: {df_raw.shape}")
except:
    try:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šCSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
        df_raw = pd.read_csv("data/raw/SOL_USDT_1d.csv", index_col=0, parse_dates=True)
        print(f"âœ… Data loaded from CSV: {df_raw.shape}")
    except:
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰
        print("âš ï¸ Creating sample data for demonstration")
        dates = pd.date_range(start='2020-01-01', end='2024-01-01', freq='D')
        np.random.seed(42)
        prices = 100 * np.cumprod(1 + np.random.randn(len(dates)) * 0.02)
        df_raw = pd.DataFrame({
            'Open': prices * (1 + np.random.randn(len(dates)) * 0.005),
            'High': prices * (1 + np.abs(np.random.randn(len(dates))) * 0.01),
            'Low': prices * (1 - np.abs(np.random.randn(len(dates))) * 0.01),
            'Close': prices,
            'Volume': np.random.randint(1000, 10000, len(dates))
        }, index=dates)

print(f"ðŸ“Š Data period: {df_raw.index[0]} to {df_raw.index[-1]}")
print(f"ðŸ”¢ Data points: {len(df_raw)}")

# é«˜åº¦ãªç‰¹å¾´é‡ç”Ÿæˆ
print("ðŸ”¬ Generating advanced features...")
try:
    from src.advanced_features import add_advanced_features
    df_features = add_advanced_features(df_raw)
    print(f"âœ… Advanced features generated: {df_features.shape}")
except ImportError:
    print("âš ï¸ Advanced features module not available - using basic features")
    # åŸºæœ¬çš„ãªç‰¹å¾´é‡ã®ã¿
    df_features = df_raw.copy()
    df_features['sma_20'] = df_features['Close'].rolling(20).mean()
    df_features['rsi'] = 100 - (100 / (1 + df_features['Close'].rolling(14).apply(
        lambda x: x[x > x.shift(1)].sum() / x[x < x.shift(1)].sum().abs()
    )))

# æ¬ æå€¤å‡¦ç†
print("ðŸ§¹ Cleaning data...")
initial_len = len(df_features)
df_features = df_features.dropna()
print(f"ðŸ”„ Dropped {initial_len - len(df_features)} rows with NaN values")

print(f"ðŸŽ¯ Final dataset: {df_features.shape}")
print("âœ… Data preparation complete!")

# ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ã®è¡¨ç¤º
print("\nðŸ“‹ Data Summary:")
df_features.info()
print(f"\nðŸ“Š Feature columns: {len(df_features.columns)}")
print(f"ðŸŽ¯ Total features: {len([col for col in df_features.columns if col not in ['Open', 'High', 'Low', 'Close', 'Volume']])}")# ðŸŒŸ ä¸–ç•Œã‚¯ãƒ©ã‚¹ãƒ¢ãƒ‡ãƒ« - ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
import os
import sys
import warnings
warnings.filterwarnings('ignore')

# GPUç’°å¢ƒãƒã‚§ãƒƒã‚¯
try:
    import torch
    print(f"ðŸ”¥ PyTorch available: {torch.__version__}")
    print(f"ðŸ’Ž CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"ðŸš€ GPU: {torch.cuda.get_device_name(0)}")
except ImportError:
    print("âš ï¸ PyTorch not available - using CPU fallback")

# ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
try:
    import mlflow
    import wandb
    print("âœ… Experiment tracking libraries available")
except ImportError:
    print("âš ï¸ Installing experiment tracking libraries...")
    # os.system("pip install mlflow wandb")

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã®è¨­å®š
sys.path.append(".")
sys.path.append("src")

print("ðŸŽ¯ World-Class Model Training Environment Ready!")# ã‚»ãƒ« 8: ãƒ¡ã‚¿æƒ…å ± JSON å‡ºåŠ›

# 6. æ¬¡å·¥ç¨‹ï¼ˆãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆï¼‰ã¸å¼•ãç¶™ããŸã‚ã®ãƒ¡ã‚¿æƒ…å ± JSON å‡ºåŠ›
print("ðŸ“‹ ãƒ¡ã‚¿æƒ…å ±ã‚’ä½œæˆä¸­...")

# ãƒ¡ã‚¿æƒ…å ±ã®ä½œæˆ
metadata = {
    "model_info": {
        "model_type": selected_model,
        "model_path": model_path,
        "feature_list_path": feature_list_path,
        "scaler_path": scaler_path if scaler is not None else None,
        "creation_date": today,
        "creation_datetime": datetime.now().isoformat()
    },
    "data_info": {
        "symbol": "SOL_USDT",
        "timeframe": "1d",
        "prediction_days": prediction_days,
        "total_samples": len(df_ml),
        "train_samples": len(X_train),
        "test_samples": len(X_test),
        "feature_count": len(feature_columns),
        "target_distribution": {
            "down_count": int(target_counts[0]),
            "up_count": int(target_counts[1]),
            "positive_rate": float(y.mean())
        }
    },
    "model_performance": {
        "accuracy": float(accuracy),
        "precision": float(precision),
        "recall": float(recall),
        "f1_score": float(f1),
        "auc": float(auc)
    },
    "training_parameters": {
        "test_size": float(test_size),
        "random_state": 42
    },
    "feature_categories": {
        "basic_ohlcv": [col for col in feature_columns if col in ['Open', 'High', 'Low', 'Close', 'Volume']],
        "technical_indicators": [col for col in feature_columns if any(indicator in col for indicator in ['SMA', 'RSI', 'MACD', 'ATR', 'bb_'])],
        "lag_features": [col for col in feature_columns if 'lag' in col],
        "pattern_features": [col for col in feature_columns if col in ['doji', 'hammer', 'shooting_star', 'engulfing_bullish', 'engulfing_bearish', 'three_white_soldiers', 'three_black_crows']]
    }
}

# ãƒ¡ã‚¿æƒ…å ±ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
metadata_path = os.path.join(model_dir, "metadata.json")
with open(metadata_path, 'w', encoding='utf-8') as f:
    json.dump(metadata, f, ensure_ascii=False, indent=2)

print(f"âœ… ãƒ¡ã‚¿æƒ…å ±ä¿å­˜å®Œäº†: {metadata_path}")

# çµæžœã‚µãƒžãƒªãƒ¼ã®è¡¨ç¤º
print(f"\nðŸŽ‰ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ã‚µãƒžãƒªãƒ¼:")
print(f"  - ãƒ¢ãƒ‡ãƒ«: {selected_model.upper()}")
print(f"  - ç²¾åº¦: {accuracy:.3f}")
print(f"  - F1ã‚¹ã‚³ã‚¢: {f1:.3f}")
print(f"  - AUC: {auc:.3f}")
print(f"  - ä¿å­˜å…ˆ: {model_dir}")
print(f"  - æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: 03_backtest.ipynb ã§ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")

# ãƒ¡ã‚¿æƒ…å ±ã®å†…å®¹ã‚’è¡¨ç¤º
print(f"\nðŸ“‹ ä¿å­˜ã•ã‚ŒãŸãƒ¡ã‚¿æƒ…å ±:")
display(pd.DataFrame([
    {"é …ç›®": "ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—", "å€¤": metadata["model_info"]["model_type"]},
    {"é …ç›®": "äºˆæ¸¬æœŸé–“", "å€¤": f"{metadata['data_info']['prediction_days']}æ—¥å¾Œ"},
    {"é …ç›®": "ã‚µãƒ³ãƒ—ãƒ«æ•°", "å€¤": metadata["data_info"]["total_samples"]},
    {"é …ç›®": "ç‰¹å¾´é‡æ•°", "å€¤": metadata["data_info"]["feature_count"]},
    {"é …ç›®": "æ­£ä¾‹çŽ‡", "å€¤": f"{metadata['data_info']['target_distribution']['positive_rate']:.3f}"},
    {"é …ç›®": "F1ã‚¹ã‚³ã‚¢", "å€¤": f"{metadata['model_performance']['f1_score']:.3f}"},
    {"é …ç›®": "AUC", "å€¤": f"{metadata['model_performance']['auc']:.3f}"}
]))# ã‚»ãƒ« 7: ãƒ¢ãƒ‡ãƒ«ä¿å­˜

# 5. å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ GDrive /data/models/{date}/model.pkl ã«ä¿å­˜
print("ðŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ä¸­...")

# æ—¥ä»˜ãƒ•ã‚©ãƒ«ãƒ€ã®ä½œæˆ
today = datetime.now().strftime("%Y%m%d")
project_path = get_project_path()
model_dir = os.path.join(project_path, "data", "models", today)
os.makedirs(model_dir, exist_ok=True)

# ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‘ã‚¹
model_path = os.path.join(model_dir, "model.pkl")

# ãƒ¢ãƒ‡ãƒ«ä¿å­˜
import pickle
with open(model_path, 'wb') as f:
    pickle.dump(model, f)

print(f"âœ… ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {model_path}")

# ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã‚‚ä¿å­˜
feature_list_path = os.path.join(model_dir, "features.json")
with open(feature_list_path, 'w', encoding='utf-8') as f:
    json.dump(feature_columns, f, ensure_ascii=False, indent=2)

print(f"âœ… ç‰¹å¾´é‡ãƒªã‚¹ãƒˆä¿å­˜å®Œäº†: {feature_list_path}")

# ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–ç”¨ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚‚ä¿å­˜ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
if selected_model in ["xgboost", "lightgbm"]:
    # Tree-based modelsã¯æ­£è¦åŒ–ä¸è¦
    scaler = None
else:
    # ä»–ã®ãƒ¢ãƒ‡ãƒ«ç”¨ã«æ­£è¦åŒ–
    scaler = StandardScaler()
    scaler.fit(X_train)
    
if scaler is not None:
    scaler_path = os.path.join(model_dir, "scaler.pkl")
    with open(scaler_path, 'wb') as f:
        pickle.dump(scaler, f)
    print(f"âœ… ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ä¿å­˜å®Œäº†: {scaler_path}")# ã‚»ãƒ« 6: ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ & ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º

# 4. å­¦ç¿’ & ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
print("ðŸ¤– ãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹...")

selected_model = model_widget.value

# ãƒ¢ãƒ‡ãƒ«å®šç¾©ã¨å­¦ç¿’
if selected_model == "xgboost":
    print("ðŸš€ XGBoost ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ä¸­...")
    model = xgb.XGBClassifier(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        random_state=42
    )
    model.fit(X_train, y_train)
    
elif selected_model == "lightgbm":
    print("âš¡ LightGBM ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ä¸­...")
    model = lgb.LGBMClassifier(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        random_state=42,
        verbose=-1
    )
    model.fit(X_train, y_train)
    
elif selected_model == "rl":
    print("ðŸ§  ç°¡æ˜“RL (ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ) ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ä¸­...")
    from sklearn.ensemble import RandomForestClassifier
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        random_state=42
    )
    model.fit(X_train, y_train)

# äºˆæ¸¬å®Ÿè¡Œ
print("ðŸ”® äºˆæ¸¬ã‚’å®Ÿè¡Œä¸­...")
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred_proba)

print("âœ… å­¦ç¿’å®Œäº†ï¼")

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹çµæžœã‚’ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
print("\nðŸ“Š å­¦ç¿’çµæžœãƒ¡ãƒˆãƒªã‚¯ã‚¹:")
metrics_df = pd.DataFrame({
    'ãƒ¡ãƒˆãƒªã‚¯ã‚¹': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC'],
    'ã‚¹ã‚³ã‚¢': [accuracy, precision, recall, f1, auc],
    'èª¬æ˜Ž': [
        'å…¨ä½“ã®æ­£è§£çŽ‡',
        'äºˆæ¸¬ã—ãŸä¸Šæ˜‡ã®ã†ã¡å®Ÿéš›ã«ä¸Šæ˜‡ã—ãŸå‰²åˆ',
        'å®Ÿéš›ã«ä¸Šæ˜‡ã—ãŸã†ã¡äºˆæ¸¬ã§ããŸå‰²åˆ',
        'Precision ã¨ Recall ã®èª¿å’Œå¹³å‡',
        'ROCæ›²ç·šã®ä¸‹å´é¢ç©'
    ]
})

# ã‚¹ã‚³ã‚¢ã‚’3æ¡ã§è¡¨ç¤º
metrics_df['ã‚¹ã‚³ã‚¢'] = metrics_df['ã‚¹ã‚³ã‚¢'].round(3)

display(metrics_df)

# åˆ†é¡žãƒ¬ãƒãƒ¼ãƒˆ
print("\nðŸ“‹ è©³ç´°åˆ†é¡žãƒ¬ãƒãƒ¼ãƒˆ:")
report = classification_report(y_test, y_pred, output_dict=True)
report_df = pd.DataFrame(report).transpose()
display(report_df.round(3))# ã‚»ãƒ« 5: ãƒ‡ãƒ¼ã‚¿æº–å‚™ & ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆ

# 4. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®ä½œæˆ
print("ðŸŽ¯ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã‚’ä½œæˆä¸­...")

# ä¾¡æ ¼å¤‰å‹•æ–¹å‘ã‚’äºˆæ¸¬ï¼ˆä¸Šæ˜‡=1, ä¸‹é™=0ï¼‰
prediction_days = target_days_widget.value
df_ml = df_features.copy()

# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆï¼ˆæŒ‡å®šæ—¥å¾Œã®ä¾¡æ ¼ãŒç¾åœ¨ã‚ˆã‚Šé«˜ã„ã‹ã©ã†ã‹ï¼‰
df_ml['target'] = (df_ml['Close'].shift(-prediction_days) > df_ml['Close']).astype(int)

# æœ€å¾Œã®Næ—¥åˆ†ã¯äºˆæ¸¬ã§ããªã„ã®ã§å‰Šé™¤
df_ml = df_ml[:-prediction_days]

print(f"âœ… ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ä½œæˆå®Œäº†")
print(f"ðŸ“Š ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ:")
target_counts = df_ml['target'].value_counts()
print(f"  - ä¸‹é™ (0): {target_counts[0]} ä»¶ ({target_counts[0]/len(df_ml)*100:.1f}%)")
print(f"  - ä¸Šæ˜‡ (1): {target_counts[1]} ä»¶ ({target_counts[1]/len(df_ml)*100:.1f}%)")

# ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†é›¢
feature_columns = [col for col in df_ml.columns if col != 'target']
X = df_ml[feature_columns]
y = df_ml['target']

print(f"\nðŸ“Š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æ¦‚è¦:")
print(f"  - ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X)}")
print(f"  - ç‰¹å¾´é‡æ•°: {len(feature_columns)}")
print(f"  - æ­£ä¾‹çŽ‡: {y.mean():.3f}")

# è¨“ç·´/ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²
test_size = test_size_widget.value
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=test_size, 
    random_state=42, 
    stratify=y
)

print(f"\nðŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²çµæžœ:")
print(f"  - è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X_train.shape}")
print(f"  - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test.shape}")
print(f"  - è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ­£ä¾‹çŽ‡: {y_train.mean():.3f}")
print(f"  - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ­£ä¾‹çŽ‡: {y_test.mean():.3f}")# ã‚»ãƒ« 4: ãƒ¢ãƒ‡ãƒ«é¸æŠž UI

# 3. ãƒ¢ãƒ‡ãƒ«é¸æŠž UI
import ipywidgets as widgets
from IPython.display import display

print("ðŸŽ›ï¸ ãƒ¢ãƒ‡ãƒ«é¸æŠž UI")

# ãƒ¢ãƒ‡ãƒ«é¸æŠžç”¨ã®ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³
model_widget = widgets.Dropdown(
    options=[
        ("XGBoost", "xgboost"),
        ("LightGBM", "lightgbm"),
        ("Reinforcement Learning (ç°¡æ˜“ç‰ˆ)", "rl")
    ],
    value="xgboost",
    description="ãƒ¢ãƒ‡ãƒ«:"
)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
test_size_widget = widgets.FloatSlider(
    value=0.2,
    min=0.1,
    max=0.4,
    step=0.05,
    description="ãƒ†ã‚¹ãƒˆã‚µã‚¤ã‚º:"
)

target_days_widget = widgets.IntSlider(
    value=1,
    min=1,
    max=5,
    step=1,
    description="äºˆæ¸¬æœŸé–“ (æ—¥):"
)

# ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã‚’è¡¨ç¤º
print("ðŸ“Š å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šã—ã¦ãã ã•ã„:")
display(model_widget)
display(test_size_widget)
display(target_days_widget)

# ç¾åœ¨ã®é¸æŠžã‚’è¡¨ç¤ºã™ã‚‹é–¢æ•°
def show_current_selection(change=None):
    print(f"\nðŸŽ¯ ç¾åœ¨ã®é¸æŠž:")
    print(f"  - ãƒ¢ãƒ‡ãƒ«: {model_widget.label}")
    print(f"  - ãƒ†ã‚¹ãƒˆã‚µã‚¤ã‚º: {test_size_widget.value}")
    print(f"  - äºˆæ¸¬æœŸé–“: {target_days_widget.value}æ—¥å¾Œ")

# é¸æŠžå¤‰æ›´æ™‚ã«è¡¨ç¤ºã‚’æ›´æ–°
model_widget.observe(show_current_selection, names='value')
test_size_widget.observe(show_current_selection, names='value')
target_days_widget.observe(show_current_selection, names='value')

# åˆæœŸé¸æŠžã‚’è¡¨ç¤º
show_current_selection()# ã‚»ãƒ« 3: ç‰¹å¾´é‡ç”Ÿæˆ

# 2. ç‰¹å¾´é‡ç”Ÿæˆ
print("ðŸ”§ ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã‚’ç”Ÿæˆä¸­...")

try:
    # utils.indicators.add_all(df) ã§ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã‚’è¿½åŠ 
    df_features = add_all(df_raw.copy())
    
    print(f"âœ… ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†: {df_features.shape}")
    print(f"ðŸ“Š è¿½åŠ ã•ã‚ŒãŸç‰¹å¾´é‡æ•°: {len(df_features.columns) - len(df_raw.columns)}")
    
    # è¿½åŠ ã•ã‚ŒãŸç‰¹å¾´é‡ã®ç¢ºèª
    new_features = [col for col in df_features.columns if col not in df_raw.columns]
    print(f"\nðŸŽ¯ æ–°ã—ã„ç‰¹å¾´é‡:")
    for i, feature in enumerate(new_features[:10]):  # æœ€åˆã®10å€‹ã‚’è¡¨ç¤º
        print(f"  {i+1}. {feature}")
    if len(new_features) > 10:
        print(f"  ... ä»– {len(new_features) - 10} å€‹")
    
    # æ¬ æå€¤ã®ç¢ºèªã¨å‡¦ç†
    print(f"\nðŸ” æ¬ æå€¤å‡¦ç†å‰: {df_features.isnull().sum().sum()} å€‹")
    df_features = df_features.dropna()
    print(f"âœ… æ¬ æå€¤å‡¦ç†å¾Œ: {df_features.shape}")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    print(f"\nðŸ“‹ ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:")
    display(df_features.head())
    
except Exception as e:
    print(f"âŒ ç‰¹å¾´é‡ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    df_features = df_raw.copy()# ã‚»ãƒ« 2: ãƒ‡ãƒ¼ã‚¿èª­è¾¼ & ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆ

# å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import pandas as pd
import numpy as np
import json
import os
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score, precision_score, recall_score
from sklearn.preprocessing import StandardScaler
import lightgbm as lgb
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå°‚ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from utils.drive_io import load_raw, save_data, get_project_path
from utils.indicators import add_all

print("âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†ï¼")

# 1. ãƒ‡ãƒ¼ã‚¿èª­è¾¼
print("\nðŸ“¥ ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’èª­è¾¼ä¸­...")

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ (01_fetch_data.ipynb ã§å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿)
try:
    df_raw = load_raw(symbol="SOL_USDT", timeframe="1d", limit=1000)
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­è¾¼å®Œäº†: {df_raw.shape}")
    print(f"ðŸ“‹ ãƒ‡ãƒ¼ã‚¿æ¦‚è¦:")
    display(df_raw.head())
except Exception as e:
    print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­è¾¼ã‚¨ãƒ©ãƒ¼: {e}")
    print("ðŸ’¡ 01_fetch_data.ipynb ã§ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ãã ã•ã„")# ã‚»ãƒ« 1: Google Drive ãƒžã‚¦ãƒ³ãƒˆ & å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

# Google Drive ã‚’ãƒžã‚¦ãƒ³ãƒˆ
from google.colab import drive
drive.mount("/content/drive")

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
!pip install scikit-learn lightgbm xgboost tensorflow pandas_ta pyti ta
!pip install ipywidgets

# ãƒ‘ã‚¹ã®è¨­å®š
import sys
sys.path.append("/content/drive/MyDrive/kucoin_bot")

print("âœ… ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†ï¼")# ðŸš€ ä¸–ç•Œã‚¯ãƒ©ã‚¹ãƒ¢ãƒ‡ãƒ« ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒŽãƒ¼ãƒˆãƒ–ãƒƒã‚¯

**ç›®çš„**: å²ä¸Šæœ€é«˜ç²¾åº¦ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é–‹ç™ºã—ã€Sharpe > 3 / MaxDD < 5% ã‚’é”æˆã™ã‚‹

## å®Ÿè¡Œãƒ•ãƒ­ãƒ¼
1. **ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—** - ç’°å¢ƒè¨­å®šã¨ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
2. **ãƒ‡ãƒ¼ã‚¿èª­è¾¼** - ç”Ÿãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
3. **é«˜åº¦ãªç‰¹å¾´é‡ç”Ÿæˆ** - TEMA, Keltner Channel, tsfresh, HurstæŒ‡æ•°ç­‰
4. **Purged Walk-Forward CV** - ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯æŽ’é™¤ã®ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
5. **ä¸–ç•Œã‚¯ãƒ©ã‚¹ãƒ¢ãƒ‡ãƒ«å­¦ç¿’** - TFT, Informer, Hybrid RLç­‰ã®å­¦ç¿’
6. **ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰** - Top 2 ãƒ¢ãƒ‡ãƒ«ã®Stacking/Blending
7. **æ‹¡å¼µãƒ¡ãƒˆãƒªã‚¯ã‚¹** - Sharpe, Calmar, VaR95ç­‰ã®è¨ˆç®—
8. **å®Ÿé¨“ç®¡ç†** - MLflow/WandB ã§ãƒ­ã‚°è¨˜éŒ²

## ç›®æ¨™
- **Sharpe ratio > 3.0**
- **Maximum Drawdown < 5%**
- **Calmar ratio > 0.6**
- **VaR95 < 2%**