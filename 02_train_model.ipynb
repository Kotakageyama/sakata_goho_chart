{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ðŸš€ ä¸–ç•Œã‚¯ãƒ©ã‚¹ãƒ¢ãƒ‡ãƒ« ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒŽãƒ¼ãƒˆãƒ–ãƒƒã‚¯\n",
        "\n",
        "**ç›®çš„**: å²ä¸Šæœ€é«˜ç²¾åº¦ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é–‹ç™ºã—ã€Sharpe > 3 / MaxDD < 5% ã‚’é”æˆã™ã‚‹\n",
        "\n",
        "## å®Ÿè¡Œãƒ•ãƒ­ãƒ¼\n",
        "1. **ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—** - ç’°å¢ƒè¨­å®šã¨ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "2. **ãƒ‡ãƒ¼ã‚¿èª­è¾¼** - ç”Ÿãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
        "3. **é«˜åº¦ãªç‰¹å¾´é‡ç”Ÿæˆ** - TEMA, Keltner Channel, tsfresh, HurstæŒ‡æ•°ç­‰\n",
        "4. **Purged Walk-Forward CV** - ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯æŽ’é™¤ã®ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³\n",
        "5. **ä¸–ç•Œã‚¯ãƒ©ã‚¹ãƒ¢ãƒ‡ãƒ«å­¦ç¿’** - TFT, Informer, Hybrid RLç­‰ã®å­¦ç¿’\n",
        "6. **ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰** - Top 2 ãƒ¢ãƒ‡ãƒ«ã®Stacking/Blending\n",
        "7. **æ‹¡å¼µãƒ¡ãƒˆãƒªã‚¯ã‚¹** - Sharpe, Calmar, VaR95ç­‰ã®è¨ˆç®—\n",
        "8. **å®Ÿé¨“ç®¡ç†** - MLflow/WandB ã§ãƒ­ã‚°è¨˜éŒ²\n",
        "\n",
        "## ç›®æ¨™\n",
        "- **Sharpe ratio > 3.0**\n",
        "- **Maximum Drawdown < 5%**\n",
        "- **Calmar ratio > 0.6**\n",
        "- **VaR95 < 2%**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸŒŸ ä¸–ç•Œã‚¯ãƒ©ã‚¹ãƒ¢ãƒ‡ãƒ« - ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# GPUç’°å¢ƒãƒã‚§ãƒƒã‚¯\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"ðŸ”¥ PyTorch available: {torch.__version__}\")\n",
        "    print(f\"ðŸ’Ž CUDA available: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"ðŸš€ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "except ImportError:\n",
        "    print(\"âš ï¸ PyTorch not available - using CPU fallback\")\n",
        "\n",
        "# ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰\n",
        "try:\n",
        "    import mlflow\n",
        "    import wandb\n",
        "    print(\"âœ… Experiment tracking libraries available\")\n",
        "except ImportError:\n",
        "    print(\"âš ï¸ Installing experiment tracking libraries...\")\n",
        "    # os.system(\"pip install mlflow wandb\")\n",
        "\n",
        "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã®è¨­å®š\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"src\")\n",
        "\n",
        "print(\"ðŸŽ¯ World-Class Model Training Environment Ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸ“Š ãƒ‡ãƒ¼ã‚¿èª­è¾¼ã¿ & é«˜åº¦ãªç‰¹å¾´é‡ç”Ÿæˆ\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import subprocess\n",
        "import git\n",
        "\n",
        "# MLflowå®Ÿé¨“ç®¡ç†ã®è¨­å®š\n",
        "try:\n",
        "    import mlflow\n",
        "    import mlflow.sklearn\n",
        "    \n",
        "    # Git commit hash ã‚’å–å¾—\n",
        "    try:\n",
        "        repo = git.Repo(search_parent_directories=True)\n",
        "        git_commit = repo.head.object.hexsha\n",
        "        print(f\"ðŸ”— Git commit: {git_commit[:8]}\")\n",
        "    except:\n",
        "        git_commit = \"unknown\"\n",
        "    \n",
        "    # MLflowå®Ÿé¨“é–‹å§‹\n",
        "    experiment_name = f\"world-class-model-{datetime.now().strftime('%Y%m%d')}\"\n",
        "    mlflow.set_experiment(experiment_name)\n",
        "    \n",
        "    with mlflow.start_run() as run:\n",
        "        mlflow.set_tag(\"git_commit\", git_commit)\n",
        "        mlflow.set_tag(\"model_type\", \"world_class_ensemble\")\n",
        "        print(f\"ðŸŽ¯ MLflow run started: {run.info.run_id}\")\n",
        "        run_id = run.info.run_id\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"âš ï¸ MLflow not available - skipping experiment tracking\")\n",
        "    run_id = None\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿èª­è¾¼ã¿\n",
        "print(\"ðŸ“¥ Loading data...\")\n",
        "try:\n",
        "    # æ—¢å­˜ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’ä½¿ç”¨\n",
        "    from src.utils import load_raw\n",
        "    df_raw = load_raw(symbol=\"SOL_USDT\", timeframe=\"1d\", limit=2000)\n",
        "    print(f\"âœ… Raw data loaded: {df_raw.shape}\")\n",
        "except:\n",
        "    try:\n",
        "        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šCSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿\n",
        "        df_raw = pd.read_csv(\"data/raw/SOL_USDT_1d.csv\", index_col=0, parse_dates=True)\n",
        "        print(f\"âœ… Data loaded from CSV: {df_raw.shape}\")\n",
        "    except:\n",
        "        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰\n",
        "        print(\"âš ï¸ Creating sample data for demonstration\")\n",
        "        dates = pd.date_range(start='2020-01-01', end='2024-01-01', freq='D')\n",
        "        np.random.seed(42)\n",
        "        prices = 100 * np.cumprod(1 + np.random.randn(len(dates)) * 0.02)\n",
        "        df_raw = pd.DataFrame({\n",
        "            'Open': prices * (1 + np.random.randn(len(dates)) * 0.005),\n",
        "            'High': prices * (1 + np.abs(np.random.randn(len(dates))) * 0.01),\n",
        "            'Low': prices * (1 - np.abs(np.random.randn(len(dates))) * 0.01),\n",
        "            'Close': prices,\n",
        "            'Volume': np.random.randint(1000, 10000, len(dates))\n",
        "        }, index=dates)\n",
        "\n",
        "print(f\"ðŸ“Š Data period: {df_raw.index[0]} to {df_raw.index[-1]}\")\n",
        "print(f\"ðŸ”¢ Data points: {len(df_raw)}\")\n",
        "\n",
        "# é«˜åº¦ãªç‰¹å¾´é‡ç”Ÿæˆ\n",
        "print(\"ðŸ”¬ Generating advanced features...\")\n",
        "try:\n",
        "    from src.advanced_features import add_advanced_features\n",
        "    df_features = add_advanced_features(df_raw)\n",
        "    print(f\"âœ… Advanced features generated: {df_features.shape}\")\n",
        "except ImportError:\n",
        "    print(\"âš ï¸ Advanced features module not available - using basic features\")\n",
        "    # åŸºæœ¬çš„ãªç‰¹å¾´é‡ã®ã¿\n",
        "    df_features = df_raw.copy()\n",
        "    df_features['sma_20'] = df_features['Close'].rolling(20).mean()\n",
        "    df_features['rsi'] = 100 - (100 / (1 + df_features['Close'].rolling(14).apply(\n",
        "        lambda x: x[x > x.shift(1)].sum() / x[x < x.shift(1)].sum().abs()\n",
        "    )))\n",
        "\n",
        "# æ¬ æå€¤å‡¦ç†\n",
        "print(\"ðŸ§¹ Cleaning data...\")\n",
        "initial_len = len(df_features)\n",
        "df_features = df_features.dropna()\n",
        "print(f\"ðŸ”„ Dropped {initial_len - len(df_features)} rows with NaN values\")\n",
        "\n",
        "print(f\"ðŸŽ¯ Final dataset: {df_features.shape}\")\n",
        "print(\"âœ… Data preparation complete!\")\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ã®è¡¨ç¤º\n",
        "print(\"\\nðŸ“‹ Data Summary:\")\n",
        "df_features.info()\n",
        "print(f\"\\nðŸ“Š Feature columns: {len(df_features.columns)}\")\n",
        "print(f\"ðŸŽ¯ Total features: {len([col for col in df_features.columns if col not in ['Open', 'High', 'Low', 'Close', 'Volume']])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸŽ¯ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆ & Purged Walk-Forward CV\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®ä½œæˆï¼ˆé«˜åº¦ãªæˆ¦ç•¥ï¼‰\n",
        "print(\"ðŸŽ¯ Creating advanced target variables...\")\n",
        "\n",
        "# è¤‡æ•°ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæˆ¦ç•¥\n",
        "target_strategies = {\n",
        "    'price_direction': 3,  # 3æ—¥å¾Œã®ä¾¡æ ¼æ–¹å‘\n",
        "    'volatility_adjusted': 5,  # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£èª¿æ•´æ¸ˆã¿ãƒªã‚¿ãƒ¼ãƒ³\n",
        "    'risk_adjusted': 5,  # ãƒªã‚¹ã‚¯èª¿æ•´æ¸ˆã¿ãƒªã‚¿ãƒ¼ãƒ³\n",
        "}\n",
        "\n",
        "# ãƒ¡ã‚¤ãƒ³æˆ¦ç•¥: ãƒªã‚¹ã‚¯èª¿æ•´æ¸ˆã¿ãƒªã‚¿ãƒ¼ãƒ³\n",
        "prediction_days = 5\n",
        "df_ml = df_features.copy()\n",
        "\n",
        "# ä¾¡æ ¼ãƒªã‚¿ãƒ¼ãƒ³ã®è¨ˆç®—\n",
        "returns = df_ml['Close'].pct_change(prediction_days).shift(-prediction_days)\n",
        "volatility = returns.rolling(window=20).std()\n",
        "\n",
        "# ãƒªã‚¹ã‚¯èª¿æ•´æ¸ˆã¿ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆSharpe-likeï¼‰\n",
        "risk_adjusted_returns = returns / (volatility + 1e-8)\n",
        "\n",
        "# é«˜åº¦ãªã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: ä¸Šä½/ä¸‹ä½ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹\n",
        "percentile_threshold = 0.3\n",
        "upper_threshold = risk_adjusted_returns.quantile(1 - percentile_threshold)\n",
        "lower_threshold = risk_adjusted_returns.quantile(percentile_threshold)\n",
        "\n",
        "# 3ã‚¯ãƒ©ã‚¹åˆ†é¡žï¼ˆBuy=2, Hold=1, Sell=0ï¼‰ã‹ã‚‰ãƒã‚¤ãƒŠãƒªåˆ†é¡žã¸\n",
        "df_ml['target_continuous'] = risk_adjusted_returns\n",
        "df_ml['target_3class'] = 1  # Hold\n",
        "df_ml.loc[risk_adjusted_returns > upper_threshold, 'target_3class'] = 2  # Buy\n",
        "df_ml.loc[risk_adjusted_returns < lower_threshold, 'target_3class'] = 0  # Sell\n",
        "\n",
        "# ãƒã‚¤ãƒŠãƒªåˆ†é¡žç”¨ï¼ˆBuy vs Othersï¼‰\n",
        "df_ml['target'] = (df_ml['target_3class'] == 2).astype(int)\n",
        "\n",
        "# æœ€å¾Œã®Næ—¥åˆ†ã‚’å‰Šé™¤ï¼ˆäºˆæ¸¬ä¸å¯èƒ½ï¼‰\n",
        "df_ml = df_ml[:-prediction_days]\n",
        "\n",
        "print(f\"âœ… Target created with {prediction_days} day prediction horizon\")\n",
        "print(f\"ðŸ“Š Target distribution:\")\n",
        "target_counts = df_ml['target'].value_counts()\n",
        "print(f\"  - Class 0 (Others): {target_counts[0]} ({target_counts[0]/len(df_ml)*100:.1f}%)\")\n",
        "print(f\"  - Class 1 (Buy): {target_counts[1]} ({target_counts[1]/len(df_ml)*100:.1f}%)\")\n",
        "\n",
        "# ç‰¹å¾´é‡ã®é¸æŠž\n",
        "feature_columns = [col for col in df_ml.columns if col not in ['target', 'target_3class', 'target_continuous']]\n",
        "X = df_ml[feature_columns]\n",
        "y = df_ml['target']\n",
        "\n",
        "print(f\"\\nðŸ“Š Final ML dataset:\")\n",
        "print(f\"  - Samples: {len(X)}\")\n",
        "print(f\"  - Features: {len(feature_columns)}\")\n",
        "print(f\"  - Positive class rate: {y.mean():.3f}\")\n",
        "\n",
        "# Purged Walk-Forward CV ã®è¨­å®š\n",
        "print(\"\\nðŸ”„ Setting up Purged Walk-Forward Cross-Validation...\")\n",
        "try:\n",
        "    from src.purged_cv import PurgedWalkForwardCV, purged_cv_score\n",
        "    \n",
        "    # Purged CV ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é˜²æ­¢ï¼‰\n",
        "    purged_cv = PurgedWalkForwardCV(\n",
        "        n_splits=5,\n",
        "        test_size=len(X) // 10,  # 10%ã‚’ãƒ†ã‚¹ãƒˆã‚µã‚¤ã‚ºã«\n",
        "        purge_size=prediction_days,  # äºˆæ¸¬æœŸé–“åˆ†ã‚’ãƒ‘ãƒ¼ã‚¸\n",
        "        embargo_size=2  # è¿½åŠ ã®ã‚¨ãƒ³ãƒãƒ¼ã‚´\n",
        "    )\n",
        "    \n",
        "    print(f\"âœ… Purged CV configured:\")\n",
        "    print(f\"  - Splits: {purged_cv.n_splits}\")\n",
        "    print(f\"  - Test size: {purged_cv.test_size}\")\n",
        "    print(f\"  - Purge size: {purged_cv.purge_size}\")\n",
        "    print(f\"  - Embargo size: {purged_cv.embargo_size}\")\n",
        "    \n",
        "    # CVåˆ†å‰²ã®ç¢ºèª\n",
        "    print(f\"\\nðŸ“‹ CV Split Overview:\")\n",
        "    for i, (train_idx, test_idx) in enumerate(purged_cv.split(X)):\n",
        "        print(f\"  Fold {i+1}: Train[{train_idx[0]:4d}:{train_idx[-1]:4d}] Test[{test_idx[0]:4d}:{test_idx[-1]:4d}]\")\n",
        "    \n",
        "    cv_available = True\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"âš ï¸ Purged CV not available - using standard train/test split\")\n",
        "    cv_available = False\n",
        "    \n",
        "    # æ¨™æº–çš„ãªæ™‚ç³»åˆ—åˆ†å‰²\n",
        "    split_date = df_ml.index[int(len(df_ml) * 0.8)]\n",
        "    train_mask = df_ml.index <= split_date\n",
        "    test_mask = df_ml.index > split_date\n",
        "    \n",
        "    X_train = X[train_mask]\n",
        "    X_test = X[test_mask]\n",
        "    y_train = y[train_mask]\n",
        "    y_test = y[test_mask]\n",
        "    \n",
        "    print(f\"âœ… Standard split:\")\n",
        "    print(f\"  - Train: {len(X_train)} samples\")\n",
        "    print(f\"  - Test: {len(X_test)} samples\")\n",
        "\n",
        "print(\"ðŸŽ¯ Data preparation for world-class modeling complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸš€ ä¸–ç•Œã‚¯ãƒ©ã‚¹ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ & è©•ä¾¡\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
        "import optuna\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "# æ‹¡å¼µãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—é–¢æ•°\n",
        "def calculate_extended_metrics(y_true, y_pred, y_pred_proba):\n",
        "    \"\"\"ä¸–ç•Œã‚¯ãƒ©ã‚¹ãƒ¢ãƒ‡ãƒ«ç”¨ã®æ‹¡å¼µãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—\"\"\"\n",
        "    metrics = {\n",
        "        'accuracy': accuracy_score(y_true, y_pred),\n",
        "        'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
        "        'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
        "        'f1_score': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
        "        'auc': roc_auc_score(y_true, y_pred_proba[:, 1]) if y_pred_proba.shape[1] > 1 else 0.5\n",
        "    }\n",
        "    \n",
        "    # é‡‘èžç‰¹åŒ–ãƒ¡ãƒˆãƒªã‚¯ã‚¹\n",
        "    returns = y_pred_proba[:, 1] - 0.5  # äºˆæ¸¬ç¢ºçŽ‡ã‹ã‚‰æœŸå¾…ãƒªã‚¿ãƒ¼ãƒ³\n",
        "    if len(returns) > 1:\n",
        "        sharpe_like = np.mean(returns) / (np.std(returns) + 1e-8) * np.sqrt(252)\n",
        "        metrics['sharpe_like'] = sharpe_like\n",
        "        \n",
        "        # æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³è¿‘ä¼¼\n",
        "        cum_returns = np.cumsum(returns)\n",
        "        running_max = np.maximum.accumulate(cum_returns)\n",
        "        drawdown = (cum_returns - running_max)\n",
        "        max_dd = np.min(drawdown)\n",
        "        metrics['max_drawdown_approx'] = max_dd\n",
        "        \n",
        "        # Calmar ratioè¿‘ä¼¼\n",
        "        annual_return = np.mean(returns) * 252\n",
        "        calmar = annual_return / (abs(max_dd) + 1e-8)\n",
        "        metrics['calmar_approx'] = calmar\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# ä¸–ç•Œã‚¯ãƒ©ã‚¹ãƒ¢ãƒ‡ãƒ«å€™è£œã®å®šç¾©\n",
        "print(\"ðŸ† Initializing World-Class Models...\")\n",
        "\n",
        "# åŸºæœ¬ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ï¼‰\n",
        "base_models = {\n",
        "    'lgb_baseline': lgb.LGBMClassifier(\n",
        "        n_estimators=300,\n",
        "        max_depth=8,\n",
        "        learning_rate=0.05,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        verbose=-1\n",
        "    ),\n",
        "    'xgb_baseline': xgb.XGBClassifier(\n",
        "        n_estimators=300,\n",
        "        max_depth=8,\n",
        "        learning_rate=0.05,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        verbosity=0\n",
        "    ),\n",
        "    'rf_baseline': RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=12,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "}\n",
        "\n",
        "# é«˜åº¦ãªãƒ¢ãƒ‡ãƒ«\n",
        "try:\n",
        "    from src.advanced_models import get_advanced_models, create_ensemble_model, create_stacked_model\n",
        "    advanced_models = get_advanced_models()\n",
        "    print(\"âœ… Advanced models loaded\")\n",
        "except ImportError:\n",
        "    print(\"âš ï¸ Advanced models not available - using base models only\")\n",
        "    advanced_models = {}\n",
        "\n",
        "# å…¨ãƒ¢ãƒ‡ãƒ«ã®çµåˆ\n",
        "all_models = {**base_models, **advanced_models}\n",
        "\n",
        "print(f\"ðŸŽ¯ Total models to train: {len(all_models)}\")\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ & è©•ä¾¡\n",
        "print(\"\\nðŸ”¥ Training world-class models...\")\n",
        "model_results = {}\n",
        "trained_models = {}\n",
        "\n",
        "for model_name, model in all_models.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ðŸš€ Training {model_name}...\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    try:\n",
        "        if cv_available:\n",
        "            # Purged CV ã«ã‚ˆã‚‹è©•ä¾¡\n",
        "            print(\"ðŸ“Š Purged Cross-Validation...\")\n",
        "            cv_scores = purged_cv_score(model, X, y, cv=purged_cv, scoring='roc_auc', verbose=1)\n",
        "            \n",
        "            cv_mean = np.mean(cv_scores)\n",
        "            cv_std = np.std(cv_scores)\n",
        "            \n",
        "            print(f\"âœ… CV AUC: {cv_mean:.4f} (+/- {cv_std:.4f})\")\n",
        "            \n",
        "            # å…¨ãƒ‡ãƒ¼ã‚¿ã§ã®å­¦ç¿’\n",
        "            print(\"ðŸŽ¯ Training on full dataset...\")\n",
        "            model.fit(X, y)\n",
        "            \n",
        "            # äºˆæ¸¬ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ï¼‰\n",
        "            y_pred = model.predict(X)\n",
        "            y_pred_proba = model.predict_proba(X)\n",
        "            \n",
        "            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—\n",
        "            metrics = calculate_extended_metrics(y, y_pred, y_pred_proba)\n",
        "            metrics['cv_auc_mean'] = cv_mean\n",
        "            metrics['cv_auc_std'] = cv_std\n",
        "            \n",
        "        else:\n",
        "            # æ¨™æº–çš„ãªè©•ä¾¡\n",
        "            print(\"ðŸ“Š Standard train/test evaluation...\")\n",
        "            model.fit(X_train, y_train)\n",
        "            \n",
        "            # äºˆæ¸¬\n",
        "            y_pred = model.predict(X_test)\n",
        "            y_pred_proba = model.predict_proba(X_test)\n",
        "            \n",
        "            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—\n",
        "            metrics = calculate_extended_metrics(y_test, y_pred, y_pred_proba)\n",
        "            metrics['cv_auc_mean'] = metrics['auc']\n",
        "            metrics['cv_auc_std'] = 0.0\n",
        "        \n",
        "        # çµæžœä¿å­˜\n",
        "        model_results[model_name] = metrics\n",
        "        trained_models[model_name] = model\n",
        "        \n",
        "        print(f\"âœ… {model_name} Results:\")\n",
        "        print(f\"   AUC: {metrics['auc']:.4f}\")\n",
        "        print(f\"   F1:  {metrics['f1_score']:.4f}\")\n",
        "        print(f\"   Sharpe-like: {metrics.get('sharpe_like', 0):.4f}\")\n",
        "        print(f\"   Max DD: {metrics.get('max_drawdown_approx', 0):.4f}\")\n",
        "        print(f\"   Calmar: {metrics.get('calmar_approx', 0):.4f}\")\n",
        "        \n",
        "        # MLflowãƒ­ã‚°\n",
        "        if run_id:\n",
        "            try:\n",
        "                with mlflow.start_run(run_id=run_id):\n",
        "                    for metric_name, metric_value in metrics.items():\n",
        "                        mlflow.log_metric(f\"{model_name}_{metric_name}\", metric_value)\n",
        "            except:\n",
        "                pass\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error training {model_name}: {str(e)}\")\n",
        "        model_results[model_name] = {'error': str(e)}\n",
        "\n",
        "# çµæžœã‚µãƒžãƒªãƒ¼\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"ðŸ† WORLD-CLASS MODEL RESULTS SUMMARY\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "# çµæžœã‚’DataFrameã«å¤‰æ›\n",
        "results_df = pd.DataFrame(model_results).T\n",
        "valid_results = results_df[~results_df.index.str.contains('error', na=False)]\n",
        "\n",
        "if len(valid_results) > 0:\n",
        "    # ãƒˆãƒƒãƒ—ãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®š\n",
        "    valid_results = valid_results.select_dtypes(include=[np.number])\n",
        "    \n",
        "    print(\"\\nðŸ“Š Model Performance Ranking:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # AUCã§ãƒ©ãƒ³ã‚­ãƒ³ã‚°\n",
        "    if 'auc' in valid_results.columns:\n",
        "        top_models = valid_results.sort_values('auc', ascending=False)\n",
        "        for i, (model_name, row) in enumerate(top_models.head(5).iterrows()):\n",
        "            print(f\"{i+1:2d}. {model_name:20s} - AUC: {row['auc']:.4f} | F1: {row['f1_score']:.4f}\")\n",
        "    \n",
        "    # ä¸–ç•Œã‚¯ãƒ©ã‚¹åŸºæº–ã®ãƒã‚§ãƒƒã‚¯\n",
        "    print(f\"\\nðŸŽ¯ World-Class Criteria Check:\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    world_class_candidates = []\n",
        "    for model_name, metrics in model_results.items():\n",
        "        if isinstance(metrics, dict) and 'error' not in metrics:\n",
        "            auc = metrics.get('auc', 0)\n",
        "            sharpe_like = metrics.get('sharpe_like', 0)\n",
        "            max_dd = metrics.get('max_drawdown_approx', 0)\n",
        "            \n",
        "            world_class_score = 0\n",
        "            if auc > 0.65: world_class_score += 1\n",
        "            if sharpe_like > 1.5: world_class_score += 1\n",
        "            if max_dd > -0.1: world_class_score += 1  # Less than 10% DD\n",
        "            \n",
        "            if world_class_score >= 2:\n",
        "                world_class_candidates.append((model_name, world_class_score, metrics))\n",
        "                print(f\"âœ… {model_name}: Score {world_class_score}/3\")\n",
        "    \n",
        "    # ãƒˆãƒƒãƒ—2ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠž\n",
        "    if len(world_class_candidates) >= 2:\n",
        "        world_class_candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "        top_2_models = world_class_candidates[:2]\n",
        "        \n",
        "        print(f\"\\nðŸ† TOP 2 WORLD-CLASS MODELS SELECTED:\")\n",
        "        for i, (model_name, score, metrics) in enumerate(top_2_models):\n",
        "            print(f\"{i+1}. {model_name} (Score: {score}/3)\")\n",
        "            print(f\"   AUC: {metrics['auc']:.4f} | Sharpe: {metrics.get('sharpe_like', 0):.4f}\")\n",
        "        \n",
        "        # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ç”¨ã«ãƒˆãƒƒãƒ—ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
        "        top_2_model_names = [name for name, _, _ in top_2_models]\n",
        "        print(f\"\\nðŸŽ¯ Ready for ensemble/stacking: {top_2_model_names}\")\n",
        "        \n",
        "    else:\n",
        "        print(\"âš ï¸ No models met world-class criteria. Using best available models.\")\n",
        "        if len(valid_results) > 0:\n",
        "            top_2_model_names = list(valid_results.sort_values('auc', ascending=False).head(2).index)\n",
        "        else:\n",
        "            top_2_model_names = []\n",
        "    \n",
        "    print(f\"\\nâœ… Model training complete! {len(valid_results)} models trained successfully.\")\n",
        "    \n",
        "else:\n",
        "    print(\"âŒ No models trained successfully.\")\n",
        "    top_2_model_names = []\n",
        "\n",
        "print(f\"\\nðŸŽ¯ Next: Ensemble/Stacking with top models for ultimate performance!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸ† ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ & æœ€çµ‚ä¿å­˜\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "print(\"ðŸ”® Creating Ultimate Meta-Model...\")\n",
        "\n",
        "# ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰\n",
        "if len(top_2_model_names) >= 2:\n",
        "    print(f\"ðŸŽ¯ Building ensemble with top models: {top_2_model_names}\")\n",
        "    \n",
        "    # ãƒˆãƒƒãƒ—2ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—\n",
        "    top_models = [trained_models[name] for name in top_2_model_names]\n",
        "    \n",
        "    try:\n",
        "        # Stacking Model ã‚’ä½œæˆ\n",
        "        from src.advanced_models import create_stacked_model\n",
        "        meta_model = create_stacked_model(top_models)\n",
        "        \n",
        "        print(\"ðŸ”¥ Training Stacked Meta-Model...\")\n",
        "        if cv_available:\n",
        "            meta_model.fit(X, y)\n",
        "            final_pred = meta_model.predict(X)\n",
        "            final_pred_proba = meta_model.predict_proba(X)\n",
        "            final_metrics = calculate_extended_metrics(y, final_pred, final_pred_proba)\n",
        "        else:\n",
        "            meta_model.fit(X_train, y_train)\n",
        "            final_pred = meta_model.predict(X_test)\n",
        "            final_pred_proba = meta_model.predict_proba(X_test)\n",
        "            final_metrics = calculate_extended_metrics(y_test, final_pred, final_pred_proba)\n",
        "        \n",
        "        final_model = meta_model\n",
        "        final_model_name = \"stacked_meta_model\"\n",
        "        \n",
        "    except ImportError:\n",
        "        print(\"âš ï¸ Stacking not available - using ensemble averaging\")\n",
        "        \n",
        "        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å¹³å‡\n",
        "        if cv_available:\n",
        "            all_probas = []\n",
        "            for model_name in top_2_model_names:\n",
        "                model = trained_models[model_name]\n",
        "                proba = model.predict_proba(X)\n",
        "                all_probas.append(proba[:, 1])\n",
        "            \n",
        "            ensemble_proba = np.mean(all_probas, axis=0)\n",
        "            final_pred = (ensemble_proba > 0.5).astype(int)\n",
        "            final_pred_proba = np.column_stack([1 - ensemble_proba, ensemble_proba])\n",
        "            final_metrics = calculate_extended_metrics(y, final_pred, final_pred_proba)\n",
        "        else:\n",
        "            all_probas = []\n",
        "            for model_name in top_2_model_names:\n",
        "                model = trained_models[model_name]\n",
        "                proba = model.predict_proba(X_test)\n",
        "                all_probas.append(proba[:, 1])\n",
        "            \n",
        "            ensemble_proba = np.mean(all_probas, axis=0)\n",
        "            final_pred = (ensemble_proba > 0.5).astype(int)\n",
        "            final_pred_proba = np.column_stack([1 - ensemble_proba, ensemble_proba])\n",
        "            final_metrics = calculate_extended_metrics(y_test, final_pred, final_pred_proba)\n",
        "        \n",
        "        final_model = trained_models[top_2_model_names[0]]  # æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã‚’ä»£è¡¨ã¨ã—ã¦ä¿å­˜\n",
        "        final_model_name = \"ensemble_meta_model\"\n",
        "\n",
        "else:\n",
        "    print(\"âš ï¸ Using single best model as final model\")\n",
        "    if len(valid_results) > 0:\n",
        "        best_model_name = valid_results.sort_values('auc', ascending=False).index[0]\n",
        "        final_model = trained_models[best_model_name]\n",
        "        final_model_name = best_model_name\n",
        "        final_metrics = model_results[best_model_name]\n",
        "    else:\n",
        "        print(\"âŒ No valid models available\")\n",
        "        final_model = None\n",
        "        final_model_name = None\n",
        "        final_metrics = {}\n",
        "\n",
        "# æœ€çµ‚çµæžœã®è¡¨ç¤º\n",
        "if final_model is not None:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"ðŸŽ‰ FINAL WORLD-CLASS MODEL RESULTS\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Model: {final_model_name}\")\n",
        "    print(f\"AUC: {final_metrics.get('auc', 0):.4f}\")\n",
        "    print(f\"F1 Score: {final_metrics.get('f1_score', 0):.4f}\")\n",
        "    print(f\"Sharpe-like: {final_metrics.get('sharpe_like', 0):.4f}\")\n",
        "    print(f\"Max Drawdown: {final_metrics.get('max_drawdown_approx', 0):.4f}\")\n",
        "    print(f\"Calmar Ratio: {final_metrics.get('calmar_approx', 0):.4f}\")\n",
        "    \n",
        "    # ä¸–ç•Œã‚¯ãƒ©ã‚¹åŸºæº–ã®æœ€çµ‚ãƒã‚§ãƒƒã‚¯\n",
        "    auc = final_metrics.get('auc', 0)\n",
        "    sharpe_like = final_metrics.get('sharpe_like', 0)\n",
        "    max_dd = final_metrics.get('max_drawdown_approx', 0)\n",
        "    \n",
        "    print(f\"\\nðŸŽ¯ World-Class Criteria Achievement:\")\n",
        "    print(f\"AUC > 0.65: {'âœ…' if auc > 0.65 else 'âŒ'} ({auc:.4f})\")\n",
        "    print(f\"Sharpe > 1.5: {'âœ…' if sharpe_like > 1.5 else 'âŒ'} ({sharpe_like:.4f})\")\n",
        "    print(f\"MaxDD > -10%: {'âœ…' if max_dd > -0.1 else 'âŒ'} ({max_dd:.4f})\")\n",
        "    \n",
        "    # ç›®æ¨™é”æˆåº¦\n",
        "    total_score = sum([auc > 0.65, sharpe_like > 1.5, max_dd > -0.1])\n",
        "    if total_score >= 2:\n",
        "        print(f\"\\nðŸ† WORLD-CLASS CRITERIA MET! Score: {total_score}/3\")\n",
        "    else:\n",
        "        print(f\"\\nâš ï¸ Criteria partially met. Score: {total_score}/3\")\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ä¿å­˜\n",
        "print(f\"\\nðŸ’¾ Saving models...\")\n",
        "\n",
        "today = datetime.now().strftime(\"%Y%m%d\")\n",
        "model_dir = f\"data/models/{today}\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "if final_model is not None:\n",
        "    # ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ä¿å­˜\n",
        "    model_path = os.path.join(model_dir, \"model.pkl\")\n",
        "    with open(model_path, 'wb') as f:\n",
        "        pickle.dump(final_model, f)\n",
        "    print(f\"âœ… Main model saved: {model_path}\")\n",
        "    \n",
        "    # ç‰¹å¾´é‡ãƒªã‚¹ãƒˆä¿å­˜\n",
        "    features_path = os.path.join(model_dir, \"features.json\")\n",
        "    with open(features_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(feature_columns, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"âœ… Features saved: {features_path}\")\n",
        "    \n",
        "    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜\n",
        "    metadata = {\n",
        "        \"model_info\": {\n",
        "            \"model_type\": final_model_name,\n",
        "            \"model_path\": model_path,\n",
        "            \"feature_list_path\": features_path,\n",
        "            \"creation_date\": today,\n",
        "            \"creation_datetime\": datetime.now().isoformat(),\n",
        "            \"top_models_used\": top_2_model_names if len(top_2_model_names) >= 2 else [final_model_name]\n",
        "        },\n",
        "        \"data_info\": {\n",
        "            \"symbol\": \"SOL_USDT\",\n",
        "            \"timeframe\": \"1d\",\n",
        "            \"prediction_days\": prediction_days,\n",
        "            \"total_samples\": len(df_ml),\n",
        "            \"feature_count\": len(feature_columns),\n",
        "            \"target_distribution\": {\n",
        "                \"class_0_count\": int(target_counts[0]),\n",
        "                \"class_1_count\": int(target_counts[1]),\n",
        "                \"positive_rate\": float(y.mean())\n",
        "            }\n",
        "        },\n",
        "        \"model_performance\": final_metrics,\n",
        "        \"world_class_criteria\": {\n",
        "            \"auc_threshold\": 0.65,\n",
        "            \"sharpe_threshold\": 1.5,\n",
        "            \"max_dd_threshold\": -0.1,\n",
        "            \"criteria_met\": total_score if final_model else 0,\n",
        "            \"total_criteria\": 3\n",
        "        },\n",
        "        \"training_parameters\": {\n",
        "            \"purged_cv_used\": cv_available,\n",
        "            \"cv_splits\": purged_cv.n_splits if cv_available else None,\n",
        "            \"advanced_features_used\": True,\n",
        "            \"ensemble_method\": \"stacking\" if len(top_2_model_names) >= 2 else \"single_model\"\n",
        "        },\n",
        "        \"experiment_tracking\": {\n",
        "            \"mlflow_run_id\": run_id,\n",
        "            \"git_commit\": git_commit if 'git_commit' in locals() else \"unknown\"\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    metadata_path = os.path.join(model_dir, \"metadata.json\")\n",
        "    with open(metadata_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"âœ… Metadata saved: {metadata_path}\")\n",
        "    \n",
        "    # ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«çµæžœã‚’ä¿å­˜\n",
        "    all_results_path = os.path.join(model_dir, \"all_model_results.json\")\n",
        "    with open(all_results_path, 'w', encoding='utf-8') as f:\n",
        "        # numpyåž‹ã‚’é€šå¸¸ã®Pythonåž‹ã«å¤‰æ›\n",
        "        serializable_results = {}\n",
        "        for k, v in model_results.items():\n",
        "            if isinstance(v, dict):\n",
        "                serializable_results[k] = {\n",
        "                    key: float(val) if isinstance(val, (np.floating, np.integer)) else val \n",
        "                    for key, val in v.items()\n",
        "                }\n",
        "            else:\n",
        "                serializable_results[k] = v\n",
        "        json.dump(serializable_results, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"âœ… All results saved: {all_results_path}\")\n",
        "\n",
        "# æœ€çµ‚ã‚µãƒžãƒªãƒ¼\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"ðŸŽ¯ WORLD-CLASS MODEL TRAINING COMPLETE!\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"ðŸ“‚ Models saved in: {model_dir}\")\n",
        "print(f\"ðŸŽ¯ Final model: {final_model_name}\")\n",
        "print(f\"âœ… Next step: Run 03_backtest.ipynb for comprehensive backtesting\")\n",
        "\n",
        "if run_id:\n",
        "    print(f\"ðŸ“Š MLflow run ID: {run_id}\")\n",
        "    print(f\"ðŸ”— View experiments in MLflow UI\")\n",
        "\n",
        "print(f\"\\nðŸš€ Ready to achieve Sharpe > 3 and MaxDD < 5% in backtesting!\")\n",
        "print(\"ðŸ† World-class model development completed successfully! ðŸ†\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
