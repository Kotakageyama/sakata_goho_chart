{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 🚀 世界クラスモデル トレーニングノートブック\n",
        "\n",
        "**目的**: 史上最高精度のアルゴリズムを開発し、Sharpe > 3 / MaxDD < 5% を達成する\n",
        "\n",
        "## 実行フロー\n",
        "1. **セットアップ** - 環境設定と依存関係のインストール\n",
        "2. **データ読込** - 生データの読み込み\n",
        "3. **高度な特徴量生成** - TEMA, Keltner Channel, tsfresh, Hurst指数等\n",
        "4. **Purged Walk-Forward CV** - データリーク排除のクロスバリデーション\n",
        "5. **世界クラスモデル学習** - TFT, Informer, Hybrid RL等の学習\n",
        "6. **メタモデル構築** - Top 2 モデルのStacking/Blending\n",
        "7. **拡張メトリクス** - Sharpe, Calmar, VaR95等の計算\n",
        "8. **実験管理** - MLflow/WandB でログ記録\n",
        "\n",
        "## 目標\n",
        "- **Sharpe ratio > 3.0**\n",
        "- **Maximum Drawdown < 5%**\n",
        "- **Calmar ratio > 0.6**\n",
        "- **VaR95 < 2%**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🌟 世界クラスモデル - セットアップ\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# GPU環境チェック\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"🔥 PyTorch available: {torch.__version__}\")\n",
        "    print(f\"💎 CUDA available: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"🚀 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "except ImportError:\n",
        "    print(\"⚠️ PyTorch not available - using CPU fallback\")\n",
        "\n",
        "# 依存関係のインストール（必要に応じて）\n",
        "try:\n",
        "    import mlflow\n",
        "    import wandb\n",
        "    print(\"✅ Experiment tracking libraries available\")\n",
        "except ImportError:\n",
        "    print(\"⚠️ Installing experiment tracking libraries...\")\n",
        "    # os.system(\"pip install mlflow wandb\")\n",
        "\n",
        "# プロジェクトパスの設定\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"src\")\n",
        "\n",
        "print(\"🎯 World-Class Model Training Environment Ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📊 データ読込み & 高度な特徴量生成\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import subprocess\n",
        "import git\n",
        "\n",
        "# MLflow実験管理の設定\n",
        "try:\n",
        "    import mlflow\n",
        "    import mlflow.sklearn\n",
        "    \n",
        "    # Git commit hash を取得\n",
        "    try:\n",
        "        repo = git.Repo(search_parent_directories=True)\n",
        "        git_commit = repo.head.object.hexsha\n",
        "        print(f\"🔗 Git commit: {git_commit[:8]}\")\n",
        "    except:\n",
        "        git_commit = \"unknown\"\n",
        "    \n",
        "    # MLflow実験開始\n",
        "    experiment_name = f\"world-class-model-{datetime.now().strftime('%Y%m%d')}\"\n",
        "    mlflow.set_experiment(experiment_name)\n",
        "    \n",
        "    with mlflow.start_run() as run:\n",
        "        mlflow.set_tag(\"git_commit\", git_commit)\n",
        "        mlflow.set_tag(\"model_type\", \"world_class_ensemble\")\n",
        "        print(f\"🎯 MLflow run started: {run.info.run_id}\")\n",
        "        run_id = run.info.run_id\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"⚠️ MLflow not available - skipping experiment tracking\")\n",
        "    run_id = None\n",
        "\n",
        "# データ読込み\n",
        "print(\"📥 Loading data...\")\n",
        "try:\n",
        "    # 既存のユーティリティを使用\n",
        "    from src.utils import load_raw\n",
        "    df_raw = load_raw(symbol=\"SOL_USDT\", timeframe=\"1d\", limit=2000)\n",
        "    print(f\"✅ Raw data loaded: {df_raw.shape}\")\n",
        "except:\n",
        "    try:\n",
        "        # フォールバック：CSVファイルから読み込み\n",
        "        df_raw = pd.read_csv(\"data/raw/SOL_USDT_1d.csv\", index_col=0, parse_dates=True)\n",
        "        print(f\"✅ Data loaded from CSV: {df_raw.shape}\")\n",
        "    except:\n",
        "        # サンプルデータ作成（デモ用）\n",
        "        print(\"⚠️ Creating sample data for demonstration\")\n",
        "        dates = pd.date_range(start='2020-01-01', end='2024-01-01', freq='D')\n",
        "        np.random.seed(42)\n",
        "        prices = 100 * np.cumprod(1 + np.random.randn(len(dates)) * 0.02)\n",
        "        df_raw = pd.DataFrame({\n",
        "            'Open': prices * (1 + np.random.randn(len(dates)) * 0.005),\n",
        "            'High': prices * (1 + np.abs(np.random.randn(len(dates))) * 0.01),\n",
        "            'Low': prices * (1 - np.abs(np.random.randn(len(dates))) * 0.01),\n",
        "            'Close': prices,\n",
        "            'Volume': np.random.randint(1000, 10000, len(dates))\n",
        "        }, index=dates)\n",
        "\n",
        "print(f\"📊 Data period: {df_raw.index[0]} to {df_raw.index[-1]}\")\n",
        "print(f\"🔢 Data points: {len(df_raw)}\")\n",
        "\n",
        "# 高度な特徴量生成\n",
        "print(\"🔬 Generating advanced features...\")\n",
        "try:\n",
        "    from src.advanced_features import add_advanced_features\n",
        "    df_features = add_advanced_features(df_raw)\n",
        "    print(f\"✅ Advanced features generated: {df_features.shape}\")\n",
        "except ImportError:\n",
        "    print(\"⚠️ Advanced features module not available - using basic features\")\n",
        "    # 基本的な特徴量のみ\n",
        "    df_features = df_raw.copy()\n",
        "    df_features['sma_20'] = df_features['Close'].rolling(20).mean()\n",
        "    df_features['rsi'] = 100 - (100 / (1 + df_features['Close'].rolling(14).apply(\n",
        "        lambda x: x[x > x.shift(1)].sum() / x[x < x.shift(1)].sum().abs()\n",
        "    )))\n",
        "\n",
        "# 欠損値処理\n",
        "print(\"🧹 Cleaning data...\")\n",
        "initial_len = len(df_features)\n",
        "df_features = df_features.dropna()\n",
        "print(f\"🔄 Dropped {initial_len - len(df_features)} rows with NaN values\")\n",
        "\n",
        "print(f\"🎯 Final dataset: {df_features.shape}\")\n",
        "print(\"✅ Data preparation complete!\")\n",
        "\n",
        "# データ概要の表示\n",
        "print(\"\\n📋 Data Summary:\")\n",
        "df_features.info()\n",
        "print(f\"\\n📊 Feature columns: {len(df_features.columns)}\")\n",
        "print(f\"🎯 Total features: {len([col for col in df_features.columns if col not in ['Open', 'High', 'Low', 'Close', 'Volume']])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🎯 ターゲット作成 & Purged Walk-Forward CV\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ターゲット変数の作成（高度な戦略）\n",
        "print(\"🎯 Creating advanced target variables...\")\n",
        "\n",
        "# 複数のターゲット戦略\n",
        "target_strategies = {\n",
        "    'price_direction': 3,  # 3日後の価格方向\n",
        "    'volatility_adjusted': 5,  # ボラティリティ調整済みリターン\n",
        "    'risk_adjusted': 5,  # リスク調整済みリターン\n",
        "}\n",
        "\n",
        "# メイン戦略: リスク調整済みリターン\n",
        "prediction_days = 5\n",
        "df_ml = df_features.copy()\n",
        "\n",
        "# 価格リターンの計算\n",
        "returns = df_ml['Close'].pct_change(prediction_days).shift(-prediction_days)\n",
        "volatility = returns.rolling(window=20).std()\n",
        "\n",
        "# リスク調整済みリターン（Sharpe-like）\n",
        "risk_adjusted_returns = returns / (volatility + 1e-8)\n",
        "\n",
        "# 高度なターゲット: 上位/下位パーセンタイルベース\n",
        "percentile_threshold = 0.3\n",
        "upper_threshold = risk_adjusted_returns.quantile(1 - percentile_threshold)\n",
        "lower_threshold = risk_adjusted_returns.quantile(percentile_threshold)\n",
        "\n",
        "# 3クラス分類（Buy=2, Hold=1, Sell=0）からバイナリ分類へ\n",
        "df_ml['target_continuous'] = risk_adjusted_returns\n",
        "df_ml['target_3class'] = 1  # Hold\n",
        "df_ml.loc[risk_adjusted_returns > upper_threshold, 'target_3class'] = 2  # Buy\n",
        "df_ml.loc[risk_adjusted_returns < lower_threshold, 'target_3class'] = 0  # Sell\n",
        "\n",
        "# バイナリ分類用（Buy vs Others）\n",
        "df_ml['target'] = (df_ml['target_3class'] == 2).astype(int)\n",
        "\n",
        "# 最後のN日分を削除（予測不可能）\n",
        "df_ml = df_ml[:-prediction_days]\n",
        "\n",
        "print(f\"✅ Target created with {prediction_days} day prediction horizon\")\n",
        "print(f\"📊 Target distribution:\")\n",
        "target_counts = df_ml['target'].value_counts()\n",
        "print(f\"  - Class 0 (Others): {target_counts[0]} ({target_counts[0]/len(df_ml)*100:.1f}%)\")\n",
        "print(f\"  - Class 1 (Buy): {target_counts[1]} ({target_counts[1]/len(df_ml)*100:.1f}%)\")\n",
        "\n",
        "# 特徴量の選択\n",
        "feature_columns = [col for col in df_ml.columns if col not in ['target', 'target_3class', 'target_continuous']]\n",
        "X = df_ml[feature_columns]\n",
        "y = df_ml['target']\n",
        "\n",
        "print(f\"\\n📊 Final ML dataset:\")\n",
        "print(f\"  - Samples: {len(X)}\")\n",
        "print(f\"  - Features: {len(feature_columns)}\")\n",
        "print(f\"  - Positive class rate: {y.mean():.3f}\")\n",
        "\n",
        "# Purged Walk-Forward CV の設定\n",
        "print(\"\\n🔄 Setting up Purged Walk-Forward Cross-Validation...\")\n",
        "try:\n",
        "    from src.purged_cv import PurgedWalkForwardCV, purged_cv_score\n",
        "    \n",
        "    # Purged CV パラメータ（データリーク防止）\n",
        "    purged_cv = PurgedWalkForwardCV(\n",
        "        n_splits=5,\n",
        "        test_size=len(X) // 10,  # 10%をテストサイズに\n",
        "        purge_size=prediction_days,  # 予測期間分をパージ\n",
        "        embargo_size=2  # 追加のエンバーゴ\n",
        "    )\n",
        "    \n",
        "    print(f\"✅ Purged CV configured:\")\n",
        "    print(f\"  - Splits: {purged_cv.n_splits}\")\n",
        "    print(f\"  - Test size: {purged_cv.test_size}\")\n",
        "    print(f\"  - Purge size: {purged_cv.purge_size}\")\n",
        "    print(f\"  - Embargo size: {purged_cv.embargo_size}\")\n",
        "    \n",
        "    # CV分割の確認\n",
        "    print(f\"\\n📋 CV Split Overview:\")\n",
        "    for i, (train_idx, test_idx) in enumerate(purged_cv.split(X)):\n",
        "        print(f\"  Fold {i+1}: Train[{train_idx[0]:4d}:{train_idx[-1]:4d}] Test[{test_idx[0]:4d}:{test_idx[-1]:4d}]\")\n",
        "    \n",
        "    cv_available = True\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"⚠️ Purged CV not available - using standard train/test split\")\n",
        "    cv_available = False\n",
        "    \n",
        "    # 標準的な時系列分割\n",
        "    split_date = df_ml.index[int(len(df_ml) * 0.8)]\n",
        "    train_mask = df_ml.index <= split_date\n",
        "    test_mask = df_ml.index > split_date\n",
        "    \n",
        "    X_train = X[train_mask]\n",
        "    X_test = X[test_mask]\n",
        "    y_train = y[train_mask]\n",
        "    y_test = y[test_mask]\n",
        "    \n",
        "    print(f\"✅ Standard split:\")\n",
        "    print(f\"  - Train: {len(X_train)} samples\")\n",
        "    print(f\"  - Test: {len(X_test)} samples\")\n",
        "\n",
        "print(\"🎯 Data preparation for world-class modeling complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🚀 世界クラスモデル学習 & 評価\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
        "import optuna\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "# 拡張メトリクス計算関数\n",
        "def calculate_extended_metrics(y_true, y_pred, y_pred_proba):\n",
        "    \"\"\"世界クラスモデル用の拡張メトリクス計算\"\"\"\n",
        "    metrics = {\n",
        "        'accuracy': accuracy_score(y_true, y_pred),\n",
        "        'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
        "        'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
        "        'f1_score': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
        "        'auc': roc_auc_score(y_true, y_pred_proba[:, 1]) if y_pred_proba.shape[1] > 1 else 0.5\n",
        "    }\n",
        "    \n",
        "    # 金融特化メトリクス\n",
        "    returns = y_pred_proba[:, 1] - 0.5  # 予測確率から期待リターン\n",
        "    if len(returns) > 1:\n",
        "        sharpe_like = np.mean(returns) / (np.std(returns) + 1e-8) * np.sqrt(252)\n",
        "        metrics['sharpe_like'] = sharpe_like\n",
        "        \n",
        "        # 最大ドローダウン近似\n",
        "        cum_returns = np.cumsum(returns)\n",
        "        running_max = np.maximum.accumulate(cum_returns)\n",
        "        drawdown = (cum_returns - running_max)\n",
        "        max_dd = np.min(drawdown)\n",
        "        metrics['max_drawdown_approx'] = max_dd\n",
        "        \n",
        "        # Calmar ratio近似\n",
        "        annual_return = np.mean(returns) * 252\n",
        "        calmar = annual_return / (abs(max_dd) + 1e-8)\n",
        "        metrics['calmar_approx'] = calmar\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# 世界クラスモデル候補の定義\n",
        "print(\"🏆 Initializing World-Class Models...\")\n",
        "\n",
        "# 基本モデル（ベンチマーク）\n",
        "base_models = {\n",
        "    'lgb_baseline': lgb.LGBMClassifier(\n",
        "        n_estimators=300,\n",
        "        max_depth=8,\n",
        "        learning_rate=0.05,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        verbose=-1\n",
        "    ),\n",
        "    'xgb_baseline': xgb.XGBClassifier(\n",
        "        n_estimators=300,\n",
        "        max_depth=8,\n",
        "        learning_rate=0.05,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        verbosity=0\n",
        "    ),\n",
        "    'rf_baseline': RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=12,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "}\n",
        "\n",
        "# 高度なモデル\n",
        "try:\n",
        "    from src.advanced_models import get_advanced_models, create_ensemble_model, create_stacked_model\n",
        "    advanced_models = get_advanced_models()\n",
        "    print(\"✅ Advanced models loaded\")\n",
        "except ImportError:\n",
        "    print(\"⚠️ Advanced models not available - using base models only\")\n",
        "    advanced_models = {}\n",
        "\n",
        "# 全モデルの結合\n",
        "all_models = {**base_models, **advanced_models}\n",
        "\n",
        "print(f\"🎯 Total models to train: {len(all_models)}\")\n",
        "\n",
        "# モデル学習 & 評価\n",
        "print(\"\\n🔥 Training world-class models...\")\n",
        "model_results = {}\n",
        "trained_models = {}\n",
        "\n",
        "for model_name, model in all_models.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"🚀 Training {model_name}...\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    try:\n",
        "        if cv_available:\n",
        "            # Purged CV による評価\n",
        "            print(\"📊 Purged Cross-Validation...\")\n",
        "            cv_scores = purged_cv_score(model, X, y, cv=purged_cv, scoring='roc_auc', verbose=1)\n",
        "            \n",
        "            cv_mean = np.mean(cv_scores)\n",
        "            cv_std = np.std(cv_scores)\n",
        "            \n",
        "            print(f\"✅ CV AUC: {cv_mean:.4f} (+/- {cv_std:.4f})\")\n",
        "            \n",
        "            # 全データでの学習\n",
        "            print(\"🎯 Training on full dataset...\")\n",
        "            model.fit(X, y)\n",
        "            \n",
        "            # 予測（全データ）\n",
        "            y_pred = model.predict(X)\n",
        "            y_pred_proba = model.predict_proba(X)\n",
        "            \n",
        "            # メトリクス計算\n",
        "            metrics = calculate_extended_metrics(y, y_pred, y_pred_proba)\n",
        "            metrics['cv_auc_mean'] = cv_mean\n",
        "            metrics['cv_auc_std'] = cv_std\n",
        "            \n",
        "        else:\n",
        "            # 標準的な評価\n",
        "            print(\"📊 Standard train/test evaluation...\")\n",
        "            model.fit(X_train, y_train)\n",
        "            \n",
        "            # 予測\n",
        "            y_pred = model.predict(X_test)\n",
        "            y_pred_proba = model.predict_proba(X_test)\n",
        "            \n",
        "            # メトリクス計算\n",
        "            metrics = calculate_extended_metrics(y_test, y_pred, y_pred_proba)\n",
        "            metrics['cv_auc_mean'] = metrics['auc']\n",
        "            metrics['cv_auc_std'] = 0.0\n",
        "        \n",
        "        # 結果保存\n",
        "        model_results[model_name] = metrics\n",
        "        trained_models[model_name] = model\n",
        "        \n",
        "        print(f\"✅ {model_name} Results:\")\n",
        "        print(f\"   AUC: {metrics['auc']:.4f}\")\n",
        "        print(f\"   F1:  {metrics['f1_score']:.4f}\")\n",
        "        print(f\"   Sharpe-like: {metrics.get('sharpe_like', 0):.4f}\")\n",
        "        print(f\"   Max DD: {metrics.get('max_drawdown_approx', 0):.4f}\")\n",
        "        print(f\"   Calmar: {metrics.get('calmar_approx', 0):.4f}\")\n",
        "        \n",
        "        # MLflowログ\n",
        "        if run_id:\n",
        "            try:\n",
        "                with mlflow.start_run(run_id=run_id):\n",
        "                    for metric_name, metric_value in metrics.items():\n",
        "                        mlflow.log_metric(f\"{model_name}_{metric_name}\", metric_value)\n",
        "            except:\n",
        "                pass\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error training {model_name}: {str(e)}\")\n",
        "        model_results[model_name] = {'error': str(e)}\n",
        "\n",
        "# 結果サマリー\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"🏆 WORLD-CLASS MODEL RESULTS SUMMARY\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "# 結果をDataFrameに変換\n",
        "results_df = pd.DataFrame(model_results).T\n",
        "valid_results = results_df[~results_df.index.str.contains('error', na=False)]\n",
        "\n",
        "if len(valid_results) > 0:\n",
        "    # トップモデルを特定\n",
        "    valid_results = valid_results.select_dtypes(include=[np.number])\n",
        "    \n",
        "    print(\"\\n📊 Model Performance Ranking:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # AUCでランキング\n",
        "    if 'auc' in valid_results.columns:\n",
        "        top_models = valid_results.sort_values('auc', ascending=False)\n",
        "        for i, (model_name, row) in enumerate(top_models.head(5).iterrows()):\n",
        "            print(f\"{i+1:2d}. {model_name:20s} - AUC: {row['auc']:.4f} | F1: {row['f1_score']:.4f}\")\n",
        "    \n",
        "    # 世界クラス基準のチェック\n",
        "    print(f\"\\n🎯 World-Class Criteria Check:\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    world_class_candidates = []\n",
        "    for model_name, metrics in model_results.items():\n",
        "        if isinstance(metrics, dict) and 'error' not in metrics:\n",
        "            auc = metrics.get('auc', 0)\n",
        "            sharpe_like = metrics.get('sharpe_like', 0)\n",
        "            max_dd = metrics.get('max_drawdown_approx', 0)\n",
        "            \n",
        "            world_class_score = 0\n",
        "            if auc > 0.65: world_class_score += 1\n",
        "            if sharpe_like > 1.5: world_class_score += 1\n",
        "            if max_dd > -0.1: world_class_score += 1  # Less than 10% DD\n",
        "            \n",
        "            if world_class_score >= 2:\n",
        "                world_class_candidates.append((model_name, world_class_score, metrics))\n",
        "                print(f\"✅ {model_name}: Score {world_class_score}/3\")\n",
        "    \n",
        "    # トップ2モデルを選択\n",
        "    if len(world_class_candidates) >= 2:\n",
        "        world_class_candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "        top_2_models = world_class_candidates[:2]\n",
        "        \n",
        "        print(f\"\\n🏆 TOP 2 WORLD-CLASS MODELS SELECTED:\")\n",
        "        for i, (model_name, score, metrics) in enumerate(top_2_models):\n",
        "            print(f\"{i+1}. {model_name} (Score: {score}/3)\")\n",
        "            print(f\"   AUC: {metrics['auc']:.4f} | Sharpe: {metrics.get('sharpe_like', 0):.4f}\")\n",
        "        \n",
        "        # 次のステップ用にトップモデルを保存\n",
        "        top_2_model_names = [name for name, _, _ in top_2_models]\n",
        "        print(f\"\\n🎯 Ready for ensemble/stacking: {top_2_model_names}\")\n",
        "        \n",
        "    else:\n",
        "        print(\"⚠️ No models met world-class criteria. Using best available models.\")\n",
        "        if len(valid_results) > 0:\n",
        "            top_2_model_names = list(valid_results.sort_values('auc', ascending=False).head(2).index)\n",
        "        else:\n",
        "            top_2_model_names = []\n",
        "    \n",
        "    print(f\"\\n✅ Model training complete! {len(valid_results)} models trained successfully.\")\n",
        "    \n",
        "else:\n",
        "    print(\"❌ No models trained successfully.\")\n",
        "    top_2_model_names = []\n",
        "\n",
        "print(f\"\\n🎯 Next: Ensemble/Stacking with top models for ultimate performance!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🏆 メタモデル構築 & 最終保存\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "print(\"🔮 Creating Ultimate Meta-Model...\")\n",
        "\n",
        "# メタモデルの構築\n",
        "if len(top_2_model_names) >= 2:\n",
        "    print(f\"🎯 Building ensemble with top models: {top_2_model_names}\")\n",
        "    \n",
        "    # トップ2モデルを取得\n",
        "    top_models = [trained_models[name] for name in top_2_model_names]\n",
        "    \n",
        "    try:\n",
        "        # Stacking Model を作成\n",
        "        from src.advanced_models import create_stacked_model\n",
        "        meta_model = create_stacked_model(top_models)\n",
        "        \n",
        "        print(\"🔥 Training Stacked Meta-Model...\")\n",
        "        if cv_available:\n",
        "            meta_model.fit(X, y)\n",
        "            final_pred = meta_model.predict(X)\n",
        "            final_pred_proba = meta_model.predict_proba(X)\n",
        "            final_metrics = calculate_extended_metrics(y, final_pred, final_pred_proba)\n",
        "        else:\n",
        "            meta_model.fit(X_train, y_train)\n",
        "            final_pred = meta_model.predict(X_test)\n",
        "            final_pred_proba = meta_model.predict_proba(X_test)\n",
        "            final_metrics = calculate_extended_metrics(y_test, final_pred, final_pred_proba)\n",
        "        \n",
        "        final_model = meta_model\n",
        "        final_model_name = \"stacked_meta_model\"\n",
        "        \n",
        "    except ImportError:\n",
        "        print(\"⚠️ Stacking not available - using ensemble averaging\")\n",
        "        \n",
        "        # アンサンブル平均\n",
        "        if cv_available:\n",
        "            all_probas = []\n",
        "            for model_name in top_2_model_names:\n",
        "                model = trained_models[model_name]\n",
        "                proba = model.predict_proba(X)\n",
        "                all_probas.append(proba[:, 1])\n",
        "            \n",
        "            ensemble_proba = np.mean(all_probas, axis=0)\n",
        "            final_pred = (ensemble_proba > 0.5).astype(int)\n",
        "            final_pred_proba = np.column_stack([1 - ensemble_proba, ensemble_proba])\n",
        "            final_metrics = calculate_extended_metrics(y, final_pred, final_pred_proba)\n",
        "        else:\n",
        "            all_probas = []\n",
        "            for model_name in top_2_model_names:\n",
        "                model = trained_models[model_name]\n",
        "                proba = model.predict_proba(X_test)\n",
        "                all_probas.append(proba[:, 1])\n",
        "            \n",
        "            ensemble_proba = np.mean(all_probas, axis=0)\n",
        "            final_pred = (ensemble_proba > 0.5).astype(int)\n",
        "            final_pred_proba = np.column_stack([1 - ensemble_proba, ensemble_proba])\n",
        "            final_metrics = calculate_extended_metrics(y_test, final_pred, final_pred_proba)\n",
        "        \n",
        "        final_model = trained_models[top_2_model_names[0]]  # 最高性能モデルを代表として保存\n",
        "        final_model_name = \"ensemble_meta_model\"\n",
        "\n",
        "else:\n",
        "    print(\"⚠️ Using single best model as final model\")\n",
        "    if len(valid_results) > 0:\n",
        "        best_model_name = valid_results.sort_values('auc', ascending=False).index[0]\n",
        "        final_model = trained_models[best_model_name]\n",
        "        final_model_name = best_model_name\n",
        "        final_metrics = model_results[best_model_name]\n",
        "    else:\n",
        "        print(\"❌ No valid models available\")\n",
        "        final_model = None\n",
        "        final_model_name = None\n",
        "        final_metrics = {}\n",
        "\n",
        "# 最終結果の表示\n",
        "if final_model is not None:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"🎉 FINAL WORLD-CLASS MODEL RESULTS\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Model: {final_model_name}\")\n",
        "    print(f\"AUC: {final_metrics.get('auc', 0):.4f}\")\n",
        "    print(f\"F1 Score: {final_metrics.get('f1_score', 0):.4f}\")\n",
        "    print(f\"Sharpe-like: {final_metrics.get('sharpe_like', 0):.4f}\")\n",
        "    print(f\"Max Drawdown: {final_metrics.get('max_drawdown_approx', 0):.4f}\")\n",
        "    print(f\"Calmar Ratio: {final_metrics.get('calmar_approx', 0):.4f}\")\n",
        "    \n",
        "    # 世界クラス基準の最終チェック\n",
        "    auc = final_metrics.get('auc', 0)\n",
        "    sharpe_like = final_metrics.get('sharpe_like', 0)\n",
        "    max_dd = final_metrics.get('max_drawdown_approx', 0)\n",
        "    \n",
        "    print(f\"\\n🎯 World-Class Criteria Achievement:\")\n",
        "    print(f\"AUC > 0.65: {'✅' if auc > 0.65 else '❌'} ({auc:.4f})\")\n",
        "    print(f\"Sharpe > 1.5: {'✅' if sharpe_like > 1.5 else '❌'} ({sharpe_like:.4f})\")\n",
        "    print(f\"MaxDD > -10%: {'✅' if max_dd > -0.1 else '❌'} ({max_dd:.4f})\")\n",
        "    \n",
        "    # 目標達成度\n",
        "    total_score = sum([auc > 0.65, sharpe_like > 1.5, max_dd > -0.1])\n",
        "    if total_score >= 2:\n",
        "        print(f\"\\n🏆 WORLD-CLASS CRITERIA MET! Score: {total_score}/3\")\n",
        "    else:\n",
        "        print(f\"\\n⚠️ Criteria partially met. Score: {total_score}/3\")\n",
        "\n",
        "# モデル保存\n",
        "print(f\"\\n💾 Saving models...\")\n",
        "\n",
        "today = datetime.now().strftime(\"%Y%m%d\")\n",
        "model_dir = f\"data/models/{today}\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "if final_model is not None:\n",
        "    # メインモデル保存\n",
        "    model_path = os.path.join(model_dir, \"model.pkl\")\n",
        "    with open(model_path, 'wb') as f:\n",
        "        pickle.dump(final_model, f)\n",
        "    print(f\"✅ Main model saved: {model_path}\")\n",
        "    \n",
        "    # 特徴量リスト保存\n",
        "    features_path = os.path.join(model_dir, \"features.json\")\n",
        "    with open(features_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(feature_columns, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"✅ Features saved: {features_path}\")\n",
        "    \n",
        "    # メタデータ保存\n",
        "    metadata = {\n",
        "        \"model_info\": {\n",
        "            \"model_type\": final_model_name,\n",
        "            \"model_path\": model_path,\n",
        "            \"feature_list_path\": features_path,\n",
        "            \"creation_date\": today,\n",
        "            \"creation_datetime\": datetime.now().isoformat(),\n",
        "            \"top_models_used\": top_2_model_names if len(top_2_model_names) >= 2 else [final_model_name]\n",
        "        },\n",
        "        \"data_info\": {\n",
        "            \"symbol\": \"SOL_USDT\",\n",
        "            \"timeframe\": \"1d\",\n",
        "            \"prediction_days\": prediction_days,\n",
        "            \"total_samples\": len(df_ml),\n",
        "            \"feature_count\": len(feature_columns),\n",
        "            \"target_distribution\": {\n",
        "                \"class_0_count\": int(target_counts[0]),\n",
        "                \"class_1_count\": int(target_counts[1]),\n",
        "                \"positive_rate\": float(y.mean())\n",
        "            }\n",
        "        },\n",
        "        \"model_performance\": final_metrics,\n",
        "        \"world_class_criteria\": {\n",
        "            \"auc_threshold\": 0.65,\n",
        "            \"sharpe_threshold\": 1.5,\n",
        "            \"max_dd_threshold\": -0.1,\n",
        "            \"criteria_met\": total_score if final_model else 0,\n",
        "            \"total_criteria\": 3\n",
        "        },\n",
        "        \"training_parameters\": {\n",
        "            \"purged_cv_used\": cv_available,\n",
        "            \"cv_splits\": purged_cv.n_splits if cv_available else None,\n",
        "            \"advanced_features_used\": True,\n",
        "            \"ensemble_method\": \"stacking\" if len(top_2_model_names) >= 2 else \"single_model\"\n",
        "        },\n",
        "        \"experiment_tracking\": {\n",
        "            \"mlflow_run_id\": run_id,\n",
        "            \"git_commit\": git_commit if 'git_commit' in locals() else \"unknown\"\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    metadata_path = os.path.join(model_dir, \"metadata.json\")\n",
        "    with open(metadata_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"✅ Metadata saved: {metadata_path}\")\n",
        "    \n",
        "    # すべてのモデル結果を保存\n",
        "    all_results_path = os.path.join(model_dir, \"all_model_results.json\")\n",
        "    with open(all_results_path, 'w', encoding='utf-8') as f:\n",
        "        # numpy型を通常のPython型に変換\n",
        "        serializable_results = {}\n",
        "        for k, v in model_results.items():\n",
        "            if isinstance(v, dict):\n",
        "                serializable_results[k] = {\n",
        "                    key: float(val) if isinstance(val, (np.floating, np.integer)) else val \n",
        "                    for key, val in v.items()\n",
        "                }\n",
        "            else:\n",
        "                serializable_results[k] = v\n",
        "        json.dump(serializable_results, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"✅ All results saved: {all_results_path}\")\n",
        "\n",
        "# 最終サマリー\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"🎯 WORLD-CLASS MODEL TRAINING COMPLETE!\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"📂 Models saved in: {model_dir}\")\n",
        "print(f\"🎯 Final model: {final_model_name}\")\n",
        "print(f\"✅ Next step: Run 03_backtest.ipynb for comprehensive backtesting\")\n",
        "\n",
        "if run_id:\n",
        "    print(f\"📊 MLflow run ID: {run_id}\")\n",
        "    print(f\"🔗 View experiments in MLflow UI\")\n",
        "\n",
        "print(f\"\\n🚀 Ready to achieve Sharpe > 3 and MaxDD < 5% in backtesting!\")\n",
        "print(\"🏆 World-class model development completed successfully! 🏆\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
